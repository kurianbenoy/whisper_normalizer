{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperNormalizer Base Module\n",
    "\n",
    "> OpenAI's non-english basic text normalization module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this module do?\n",
    "\n",
    "As per the text normalization/standardization approach mentioned in  Appendix Section C pp.21 in  the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `BasicTextNormalizer` does the following functionality:\n",
    "\n",
    "1. Remove any phrases between matching brackets ([, ]).\n",
    "2. Remove any phrases between matching parentheses ((, )).\n",
    "3. Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each\n",
    "character in the NFKC-normalized string starts with M, S, or P.\n",
    "4. make the text lowercase.\n",
    "5. replace any successive whitespace characters with a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# This code is from OpenAI Whisper Repository: https://github.com/openai/whisper/tree/main/whisper/normalizers\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import regex\n",
    "\n",
    "# from fastcore.foundation import add_docs\n",
    "\n",
    "\n",
    "# non-ASCII letters that are not separated by \"NFKD\" normalization\n",
    "ADDITIONAL_DIACRITICS = {\n",
    "    \"œ\": \"oe\",\n",
    "    \"Œ\": \"OE\",\n",
    "    \"ø\": \"o\",\n",
    "    \"Ø\": \"O\",\n",
    "    \"æ\": \"ae\",\n",
    "    \"Æ\": \"AE\",\n",
    "    \"ß\": \"ss\",\n",
    "    \"ẞ\": \"SS\",\n",
    "    \"đ\": \"d\",\n",
    "    \"Đ\": \"D\",\n",
    "    \"ð\": \"d\",\n",
    "    \"Ð\": \"D\",\n",
    "    \"þ\": \"th\",\n",
    "    \"Þ\": \"th\",\n",
    "    \"ł\": \"l\",\n",
    "    \"Ł\": \"L\",\n",
    "}\n",
    "\n",
    "\n",
    "def remove_symbols_and_diacritics(s: str, keep=\"\"):\n",
    "    \"\"\"\n",
    "    Replace any other markers, symbols, and punctuations with a space,\n",
    "    and drop any diacritics (category 'Mn' and some manual mappings)\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        (\n",
    "            c\n",
    "            if c in keep\n",
    "            else (\n",
    "                ADDITIONAL_DIACRITICS[c]\n",
    "                if c in ADDITIONAL_DIACRITICS\n",
    "                else (\n",
    "                    \"\"\n",
    "                    if unicodedata.category(c) == \"Mn\"\n",
    "                    else \" \"\n",
    "                    if unicodedata.category(c)[0] in \"MSP\"\n",
    "                    else c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        for c in unicodedata.normalize(\"NFKD\", s)\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_symbols(s: str):\n",
    "    \"\"\"\n",
    "    Replace any other markers, symbols, punctuations with a space, keeping diacritics\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        \" \" if unicodedata.category(c)[0] in \"MSP\" else c\n",
    "        for c in unicodedata.normalize(\"NFKC\", s)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BasicTextNormalizer:\n",
    "    \"\"\"As per the text normalization/standardization approach mentioned in  Appendix Section C pp.21 in  the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `BasicTextNormalizer` does the following functionality:\n",
    "\n",
    "        1. Remove any phrases between matching brackets ([, ]).\n",
    "        2. Remove any phrases between matching parentheses ((, )).\n",
    "        3. Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each\n",
    "        character in the NFKC-normalized string starts with M, S, or P.\n",
    "        4. make the text lowercase.\n",
    "        5. replace any successive whitespace characters with a space\n",
    "\n",
    "    Note: It's not recommended to use this function for non-english languages because it may removes vowels in languages as identified by [kavya in this tweet](https://twitter.com/kavya_manohar/status/1752574864618365059).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        remove_diacritics: bool = False,\n",
    "        split_letters: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        remove_diaciritics - Replace any other markers, symbols, and punctuations with a space and drop any diacritics\n",
    "        split_letters  - It uses a regular expression \\X to find all Unicode graphemes (extended grapheme clusters) in the string s and join them together by space\n",
    "        \"\"\"\n",
    "        self.clean = (\n",
    "            remove_symbols_and_diacritics if remove_diacritics else remove_symbols\n",
    "        )\n",
    "        self.split_letters = split_letters\n",
    "\n",
    "    def __call__(self, s: str):\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"[<\\[][^>\\]]*[>\\]]\", \"\", s)  # remove words between brackets\n",
    "        s = re.sub(r\"\\(([^)]+?)\\)\", \"\", s)  # remove words between parenthesis\n",
    "        s = self.clean(s).lower()\n",
    "\n",
    "        if self.split_letters:\n",
    "            s = \" \".join(regex.findall(r\"\\X\", s, regex.U))\n",
    "\n",
    "        s = re.sub(\n",
    "            r\"\\s+\", \" \", s\n",
    "        )  # replace any successive whitespace characters with a space\n",
    "\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# add_docs(BasicTextNormalizer, \"Initialize BasicTextNormalizer\",\n",
    "#          # remove_diacritics=\"Replace any other markers, symbols, and punctuations with a space and drop any diacritics\",\n",
    "#          # split_letters=\"It uses a regular expression \\X to find all Unicode graphemes (extended grapheme clusters) in the string s and join them together by space\",\n",
    "#          __call__=\"Call string s and apply normalizer with `BasicTextNormalizer`\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kurianbenoy/whisper_normalizer/blob/main/whisper_normalizer/basic.py#L62){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BasicTextNormalizer\n",
       "\n",
       ">      BasicTextNormalizer (remove_diacritics:bool=False,\n",
       ">                           split_letters:bool=False)\n",
       "\n",
       "As per the text normalization/standardization approach mentioned in  Appendix Section C pp.21 in  the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `BasicTextNormalizer` does the following functionality:\n",
       "\n",
       "    1. Remove any phrases between matching brackets ([, ]).\n",
       "    2. Remove any phrases between matching parentheses ((, )).\n",
       "    3. Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each\n",
       "    character in the NFKC-normalized string starts with M, S, or P.\n",
       "    4. make the text lowercase.\n",
       "    5. replace any successive whitespace characters with a space\n",
       "\n",
       "Note: It's not recommended to use this function for non-english languages because it may removes vowels in languages as identified by [kavya in this tweet](https://twitter.com/kavya_manohar/status/1752574864618365059)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kurianbenoy/whisper_normalizer/blob/main/whisper_normalizer/basic.py#L62){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BasicTextNormalizer\n",
       "\n",
       ">      BasicTextNormalizer (remove_diacritics:bool=False,\n",
       ">                           split_letters:bool=False)\n",
       "\n",
       "As per the text normalization/standardization approach mentioned in  Appendix Section C pp.21 in  the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `BasicTextNormalizer` does the following functionality:\n",
       "\n",
       "    1. Remove any phrases between matching brackets ([, ]).\n",
       "    2. Remove any phrases between matching parentheses ((, )).\n",
       "    3. Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each\n",
       "    character in the NFKC-normalized string starts with M, S, or P.\n",
       "    4. make the text lowercase.\n",
       "    5. replace any successive whitespace characters with a space\n",
       "\n",
       "Note: It's not recommended to use this function for non-english languages because it may removes vowels in languages as identified by [kavya in this tweet](https://twitter.com/kavya_manohar/status/1752574864618365059)."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(BasicTextNormalizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Basic Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'എന റ കമ പ യ ട ടറ ന എന റ ഭ ഷ'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = BasicTextNormalizer()\n",
    "normalizer(\"എന്റെ കമ്പ്യൂട്ടറിനു് എന്റെ ഭാഷ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'language is like a map that we use to navigate the world but it s also like a prison that keeps us from seeing what s beyond the walls but what if there was a way to break out of this prison to expand our map to explore new worlds with new words this is the possibility and the challenge offered by instruction tuned language models like gpt 4 a cutting edge technology that uses artificial neural networks to generate natural language texts based on user inputs gpt 4 can write anything from essays to novels to poems to tweets to code to recipes to jokes to lyrics to whatever you want it can even write things that don t exist yet things that no human has ever thought of or said before as wittgenstein s quote suggests language is a source of limitation and liberation gpt 4 pushes this idea to the extreme by giving us access to unlimited language this could be the most significant new technology in modern history because it has the potential to change many domains and industries from education to entertainment from journalism to justice from science to art these models could enable new forms of learning storytelling reporting reasoning discovery and creation they could also create new ethical social and cultural challenges that require careful reflection and regulation how we use this technology will depend on how we recognize its implications for ourselves and others this technology is a form of artificial intelligence the word intelligence derives from inter and legere to be intelligent then is to be able to choose between things to pick out what matters to read what is written intelligence is not just a quantity or a quality it is an activity a process a practice it is something that we do with our minds and our words but when we let gpt 4 do this for us are we not abdicating our intelligence are we not letting go of our ability to choose to pick out to read are we not becoming passive consumers of language instead of active producers '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text = \"\"\"Language is like a map that we use to navigate the world, but it’s also like a prison that keeps us from seeing what’s beyond the walls.\n",
    "\n",
    "But what if there was a way to break out of this prison, to expand our map, to explore new worlds with new words? This is the possibility and the challenge offered by instruction tuned language models like GPT 4, a cutting-edge technology that uses artificial neural networks to generate natural language texts based on user inputs.\n",
    "\n",
    "GPT 4 can write anything from essays to novels to poems to tweets to code to recipes to jokes to lyrics to whatever you want. It can even write things that don’t exist yet, things that no human has ever thought of or said before.\n",
    "\n",
    "As Wittgenstein’s quote suggests, language is a source of limitation and liberation. GPT 4 pushes this idea to the extreme by giving us access to unlimited language.\n",
    "\n",
    "This could be the most significant new technology in modern history because it has the potential to change many domains and industries. From education to entertainment, from journalism to justice, from science to art, these models could enable new forms of learning, storytelling, reporting, reasoning, discovery, and creation.\n",
    "\n",
    "They could also create new ethical, social, and cultural challenges that require careful reflection and regulation. How we use this technology will depend on how we recognize its implications for ourselves and others.\n",
    "\n",
    "This technology is a form of “Artificial Intelligence”. The word “intelligence” derives from inter- (“between”) and legere (“to choose, pick out, read”). To be intelligent, then, is to be able to choose between things, to pick out what matters, to read what is written. Intelligence is not just a quantity or a quality; it is an activity, a process, a practice. It is something that we do with our minds and our words.\n",
    "\n",
    "But when we let GPT 4 do this for us, are we not abdicating our intelligence? Are we not letting go of our ability to choose, to pick out, to read? Are we not becoming passive consumers of language instead of active producers?\n",
    "\"\"\"\n",
    "normalizer(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'language is like a map that we use to navigate the world but it s also like a prison that keeps us from seeing what s beyond the walls but what if there was a way to break out of this prison to expand our map to explore new worlds with new words this is the possibility and the challenge offered by instruction tuned language models like gpt 4 a cutting edge technology that uses artificial neural networks to generate natural language texts based on user inputs gpt 4 can write anything from essays to novels to poems to tweets to code to recipes to jokes to lyrics to whatever you want it can even write things that don t exist yet things that no human has ever thought of or said before as wittgenstein s quote suggests language is a source of limitation and liberation gpt 4 pushes this idea to the extreme by giving us access to unlimited language this could be the most significant new technology in modern history because it has the potential to change many domains and industries from education to entertainment from journalism to justice from science to art these models could enable new forms of learning storytelling reporting reasoning discovery and creation they could also create new ethical social and cultural challenges that require careful reflection and regulation how we use this technology will depend on how we recognize its implications for ourselves and others this technology is a form of artificial intelligence the word intelligence derives from inter and legere to be intelligent then is to be able to choose between things to pick out what matters to read what is written intelligence is not just a quantity or a quality it is an activity a process a practice it is something that we do with our minds and our words but when we let gpt 4 do this for us are we not abdicating our intelligence are we not letting go of our ability to choose to pick out to read are we not becoming passive consumers of language instead of active producers '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = BasicTextNormalizer(remove_diacritics=True)\n",
    "\n",
    "article_text = \"\"\"Language is like a map that we use to navigate the world, but it’s also like a prison that keeps us from seeing what’s beyond the walls.\n",
    "\n",
    "But what if there was a way to break out of this prison, to expand our map, to explore new worlds with new words? This is the possibility and the challenge offered by instruction tuned language models like GPT 4, a cutting-edge technology that uses artificial neural networks to generate natural language texts based on user inputs.\n",
    "\n",
    "GPT 4 can write anything from essays to novels to poems to tweets to code to recipes to jokes to lyrics to whatever you want. It can even write things that don’t exist yet, things that no human has ever thought of or said before.\n",
    "\n",
    "As Wittgenstein’s quote suggests, language is a source of limitation and liberation. GPT 4 pushes this idea to the extreme by giving us access to unlimited language.\n",
    "\n",
    "This could be the most significant new technology in modern history because it has the potential to change many domains and industries. From education to entertainment, from journalism to justice, from science to art, these models could enable new forms of learning, storytelling, reporting, reasoning, discovery, and creation.\n",
    "\n",
    "They could also create new ethical, social, and cultural challenges that require careful reflection and regulation. How we use this technology will depend on how we recognize its implications for ourselves and others.\n",
    "\n",
    "This technology is a form of “Artificial Intelligence”. The word “intelligence” derives from inter- (“between”) and legere (“to choose, pick out, read”). To be intelligent, then, is to be able to choose between things, to pick out what matters, to read what is written. Intelligence is not just a quantity or a quality; it is an activity, a process, a practice. It is something that we do with our minds and our words.\n",
    "\n",
    "But when we let GPT 4 do this for us, are we not abdicating our intelligence? Are we not letting go of our ability to choose, to pick out, to read? Are we not becoming passive consumers of language instead of active producers?\n",
    "\"\"\"\n",
    "normalizer(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
