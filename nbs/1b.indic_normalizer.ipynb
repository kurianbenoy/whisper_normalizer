{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fb7cc8-64d9-49e0-ad21-cfc74fb828f6",
   "metadata": {},
   "source": [
    "## Indic Languages Normalizer\n",
    "\n",
    "This code is from: https://github.com/anoopkunchukuttan/indic_nlp_library\n",
    "\n",
    "Also use Indic Numtowords: https://github.com/raj-sutariya/indic-num2words, https://github.com/AI4Bharat/indic-numtowords\n",
    "\n",
    "This code has been modified by Kurian to suit to Whisper-normalizer style of coding and the logic for Malayalam normalization is expanded beyond the Indic NLP library by Dr Kavya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504628a-590b-4da4-bee1-a10cca3eef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp indic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233437ec-4821-4887-8f61-222bb03ce1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f2528-480a-4671-a52f-b8f33df5db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import re\n",
    "from indic_numtowords import num2words\n",
    "\n",
    "\n",
    "from whisper_normalizer import langinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905827d9-641e-48a5-b711-242df0328c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "#  Copyright (c) 2013-present, Anoop Kunchukuttan\n",
    "#  All rights reserved.\n",
    "#\n",
    "#  This source code is licensed under the MIT license found in the\n",
    "#  LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "# Program for normalization of text written in Unicode. This is mainly geared towards Indic scripts\n",
    "#\n",
    "# @author Anoop Kunchukuttan\n",
    "#\n",
    "\n",
    "\n",
    "class NormalizerI(object):\n",
    "    \"\"\"\n",
    "    The normalizer classes do the following:\n",
    "    * Some characters have multiple Unicode codepoints. The normalizer chooses a single standard representation\n",
    "    * Some control characters are deleted\n",
    "    * While typing using the Latin keyboard, certain typical mistakes occur which are corrected by the module\n",
    "    Base class for normalizer. Performs some common normalization, which includes:\n",
    "    * Byte order mark, word joiner, etc. removal\n",
    "    * ZERO_WIDTH_NON_JOINER and ZERO_WIDTH_JOINER removal\n",
    "    * ZERO_WIDTH_SPACE and NO_BREAK_SPACE replaced by spaces\n",
    "    Script specific normalizers should derive from this class and override the normalize() method.\n",
    "    They can call the super class 'normalize() method to avail of the common normalization\n",
    "    \"\"\"\n",
    "\n",
    "    BYTE_ORDER_MARK = \"\\ufeff\"\n",
    "    BYTE_ORDER_MARK_2 = \"\\ufffe\"\n",
    "    WORD_JOINER = \"\\u2060\"\n",
    "    SOFT_HYPHEN = \"\\u00ad\"\n",
    "\n",
    "    ZERO_WIDTH_SPACE = \"\\u200b\"\n",
    "    NO_BREAK_SPACE = \"\\u00a0\"\n",
    "\n",
    "    ZERO_WIDTH_NON_JOINER = \"\\u200c\"\n",
    "    ZERO_WIDTH_JOINER = \"\\u200d\"\n",
    "\n",
    "    def _normalize_punctuations(self, text):\n",
    "        \"\"\"\n",
    "        Normalize punctuations.\n",
    "        Applied many of the punctuation normalizations that are part of MosesNormalizer\n",
    "        from sacremoses\n",
    "        \"\"\"\n",
    "        text = text.replace(NormalizerI.BYTE_ORDER_MARK, \"\")\n",
    "        text = text.replace(\"„\", r'\"')\n",
    "        text = text.replace(\"“\", r'\"')\n",
    "        text = text.replace(\"”\", r'\"')\n",
    "        text = text.replace(\"–\", r\"-\")\n",
    "        text = text.replace(\"—\", r\" - \")\n",
    "        text = text.replace(\"´\", r\"'\")\n",
    "        text = text.replace(\"‘\", r\"'\")\n",
    "        text = text.replace(\"‚\", r\"'\")\n",
    "        text = text.replace(\"’\", r\"'\")\n",
    "        text = text.replace(\"''\", r'\"')\n",
    "        text = text.replace(\"´´\", r'\"')\n",
    "        text = text.replace(\"…\", r\"...\")\n",
    "\n",
    "        return text\n",
    "\n",
    "    def normalize(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59898a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BaseNormalizer(NormalizerI):\n",
    "    \"\"\"Common class used in most of indic languages inherit from this code.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang,\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "    ):\n",
    "        self.lang = lang\n",
    "        self.remove_nuktas = remove_nuktas\n",
    "        self.nasals_mode = nasals_mode\n",
    "        self.do_normalize_chandras = do_normalize_chandras\n",
    "        self.do_normalize_vowel_ending = do_normalize_vowel_ending\n",
    "\n",
    "        self._init_normalize_chandras()\n",
    "        self._init_normalize_nasals()\n",
    "        self._init_normalize_vowel_ending()\n",
    "        # self._init_visarga_correction()\n",
    "\n",
    "    def _init_normalize_vowel_ending(self):\n",
    "        if self.lang in langinfo.IE_LANGUAGES:\n",
    "            self.fn_vowel_ending = self._normalize_word_vowel_ending_ie\n",
    "        elif self.lang in langinfo.DRAVIDIAN_LANGUAGES:\n",
    "            self.fn_vowel_ending = self._normalize_word_vowel_ending_dravidian\n",
    "        else:\n",
    "            self.fn_vowel_ending = lambda x: x\n",
    "\n",
    "    def _init_normalize_chandras(self):\n",
    "        substitution_offsets = [\n",
    "            [0x0D, 0x0F],  # chandra e, independent\n",
    "            [0x11, 0x13],  # chandra o, independent\n",
    "            [0x45, 0x47],  # chandra e , 0xde],pendent\n",
    "            [0x49, 0x4B],  # chandra o , 0xde],pendent\n",
    "            # [0x72 , 0x0f], # mr: chandra e, independent\n",
    "            [0x00, 0x02],  # chandrabindu\n",
    "            [0x01, 0x02],  # chandrabindu\n",
    "        ]\n",
    "\n",
    "        self.chandra_substitutions = [\n",
    "            (\n",
    "                langinfo.offset_to_char(x[0], self.lang),\n",
    "                langinfo.offset_to_char(x[1], self.lang),\n",
    "            )\n",
    "            for x in substitution_offsets\n",
    "        ]\n",
    "\n",
    "    def _normalize_chandras(self, text):\n",
    "        for match, repl in self.chandra_substitutions:\n",
    "            text = text.replace(match, repl)\n",
    "        return text\n",
    "\n",
    "    def _init_to_anusvaara_strict(self):\n",
    "        \"\"\"\n",
    "        `r1_nasal=re.compile(r'\\\\u0919\\\\u094D([\\\\u0915-\\\\u0918])')`\n",
    "        \"\"\"\n",
    "\n",
    "        pat_signatures = [\n",
    "            [0x19, 0x15, 0x18],\n",
    "            [0x1E, 0x1A, 0x1D],\n",
    "            [0x23, 0x1F, 0x22],\n",
    "            [0x28, 0x24, 0x27],\n",
    "            [0x29, 0x24, 0x27],\n",
    "            [0x2E, 0x2A, 0x2D],\n",
    "        ]\n",
    "\n",
    "        halant_offset = 0x4D\n",
    "        anusvaara_offset = 0x02\n",
    "\n",
    "        pats = []\n",
    "\n",
    "        for pat_signature in pat_signatures:\n",
    "            pat = re.compile(\n",
    "                r\"{nasal}{halant}([{start_r}-{end_r}])\".format(\n",
    "                    nasal=langinfo.offset_to_char(pat_signature[0], self.lang),\n",
    "                    halant=langinfo.offset_to_char(halant_offset, self.lang),\n",
    "                    start_r=langinfo.offset_to_char(pat_signature[1], self.lang),\n",
    "                    end_r=langinfo.offset_to_char(pat_signature[2], self.lang),\n",
    "                )\n",
    "            )\n",
    "            pats.append(pat)\n",
    "\n",
    "        repl_string = \"{anusvaara}\\\\1\".format(\n",
    "            anusvaara=langinfo.offset_to_char(anusvaara_offset, self.lang)\n",
    "        )\n",
    "\n",
    "        self.pats_repls = (pats, repl_string)\n",
    "\n",
    "    def _to_anusvaara_strict(self, text):\n",
    "        pats, repl_string = self.pats_repls\n",
    "        for pat in pats:\n",
    "            text = pat.sub(repl_string, text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _init_to_anusvaara_relaxed(self):\n",
    "        \"\"\"\n",
    "        `r1_nasal=re.compile(r'\\\\u0919\\\\u094D([\\\\u0915-\\\\u0918])')`\n",
    "        \"\"\"\n",
    "\n",
    "        nasals_list = [0x19, 0x1E, 0x23, 0x28, 0x29, 0x2E]\n",
    "        nasals_list_str = \",\".join(\n",
    "            [langinfo.offset_to_char(x, self.lang) for x in nasals_list]\n",
    "        )\n",
    "\n",
    "        halant_offset = 0x4D\n",
    "        anusvaara_offset = 0x02\n",
    "\n",
    "        pat = re.compile(\n",
    "            r\"[{nasals_list_str}]{halant}\".format(\n",
    "                nasals_list_str=nasals_list_str,\n",
    "                halant=langinfo.offset_to_char(halant_offset, self.lang),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        repl_string = \"{anusvaara}\".format(\n",
    "            anusvaara=langinfo.offset_to_char(anusvaara_offset, self.lang)\n",
    "        )\n",
    "\n",
    "        self.pats_repls = (pat, repl_string)\n",
    "\n",
    "    def _to_anusvaara_relaxed(self, text):\n",
    "        pat, repl_string = self.pats_repls\n",
    "        return pat.sub(repl_string, text)\n",
    "\n",
    "    def _init_to_nasal_consonants(self):\n",
    "        \"\"\"\n",
    "        `r1_nasal=re.compile(r'\\\\u0919\\\\u094D([\\\\u0915-\\\\u0918])')`\n",
    "        \"\"\"\n",
    "\n",
    "        pat_signatures = [\n",
    "            [0x19, 0x15, 0x18],\n",
    "            [0x1E, 0x1A, 0x1D],\n",
    "            [0x23, 0x1F, 0x22],\n",
    "            [0x28, 0x24, 0x27],\n",
    "            [0x29, 0x24, 0x27],\n",
    "            [0x2E, 0x2A, 0x2D],\n",
    "        ]\n",
    "\n",
    "        halant_offset = 0x4D\n",
    "        anusvaara_offset = 0x02\n",
    "\n",
    "        pats = []\n",
    "        repl_strings = []\n",
    "\n",
    "        for pat_signature in pat_signatures:\n",
    "            pat = re.compile(\n",
    "                r\"{anusvaara}([{start_r}-{end_r}])\".format(\n",
    "                    anusvaara=langinfo.offset_to_char(anusvaara_offset, self.lang),\n",
    "                    start_r=langinfo.offset_to_char(pat_signature[1], self.lang),\n",
    "                    end_r=langinfo.offset_to_char(pat_signature[2], self.lang),\n",
    "                )\n",
    "            )\n",
    "            pats.append(pat)\n",
    "            repl_string = \"{nasal}{halant}\\\\1\".format(\n",
    "                nasal=langinfo.offset_to_char(pat_signature[0], self.lang),\n",
    "                halant=langinfo.offset_to_char(halant_offset, self.lang),\n",
    "            )\n",
    "            repl_strings.append(repl_string)\n",
    "\n",
    "        self.pats_repls = list(zip(pats, repl_strings))\n",
    "\n",
    "    def _to_nasal_consonants(self, text):\n",
    "        for pat, repl in self.pats_repls:\n",
    "            text = pat.sub(repl, text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _init_normalize_nasals(self):\n",
    "        if self.nasals_mode == \"to_anusvaara_strict\":\n",
    "            self._init_to_anusvaara_strict()\n",
    "        elif self.nasals_mode == \"to_anusvaara_relaxed\":\n",
    "            self._init_to_anusvaara_relaxed()\n",
    "        elif self.nasals_mode == \"to_nasal_consonants\":\n",
    "            self._init_to_nasal_consonants()\n",
    "\n",
    "    def _normalize_nasals(self, text):\n",
    "        if self.nasals_mode == \"to_anusvaara_strict\":\n",
    "            return self._to_anusvaara_strict(text)\n",
    "        elif self.nasals_mode == \"to_anusvaara_relaxed\":\n",
    "            return self._to_anusvaara_relaxed(text)\n",
    "        elif self.nasals_mode == \"to_nasal_consonants\":\n",
    "            return self._to_nasal_consonants(text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def _normalize_word_vowel_ending_dravidian(self, word):\n",
    "        \"\"\"\n",
    "        for Dravidian\n",
    "        - consonant ending: add 'a' ki maatra\n",
    "        - halant ending: no change\n",
    "        - 'a' ki maatra: no change\n",
    "        \"\"\"\n",
    "        if len(word) > 0 and langinfo.is_consonant(word[-1], self.lang):\n",
    "            return word + langinfo.offset_to_char(0x3E, self.lang)\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def _normalize_word_vowel_ending_ie(self, word):\n",
    "        \"\"\"\n",
    "        for IE\n",
    "        - consonant ending: add halant\n",
    "        - halant ending: no change\n",
    "        - 'a' ki maatra: no change\n",
    "        \"\"\"\n",
    "        if len(word) > 0 and langinfo.is_consonant(word[-1], self.lang):\n",
    "            return word + langinfo.offset_to_char(langinfo.HALANTA_OFFSET, self.lang)\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def _normalize_vowel_ending(self, text):\n",
    "        return \" \".join([self.fn_vowel_ending(w) for w in text.split(\" \")])\n",
    "\n",
    "    def normalize(self, text):\n",
    "        \"\"\"\n",
    "        Method to be implemented for normalization for each script\n",
    "        \"\"\"\n",
    "        text = text.replace(NormalizerI.BYTE_ORDER_MARK, \"\")\n",
    "        text = text.replace(NormalizerI.BYTE_ORDER_MARK_2, \"\")\n",
    "        text = text.replace(NormalizerI.WORD_JOINER, \"\")\n",
    "        text = text.replace(NormalizerI.SOFT_HYPHEN, \"\")\n",
    "\n",
    "        text = text.replace(NormalizerI.ZERO_WIDTH_SPACE, \" \")  # ??\n",
    "        text = text.replace(NormalizerI.NO_BREAK_SPACE, \" \")\n",
    "\n",
    "        text = text.replace(NormalizerI.ZERO_WIDTH_NON_JOINER, \"\")\n",
    "        text = text.replace(NormalizerI.ZERO_WIDTH_JOINER, \"\")\n",
    "\n",
    "        text = self._normalize_punctuations(text)\n",
    "\n",
    "        if self.do_normalize_chandras:\n",
    "            text = self._normalize_chandras(text)\n",
    "        text = self._normalize_nasals(text)\n",
    "        if self.do_normalize_vowel_ending:\n",
    "            text = self._normalize_vowel_ending(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def get_char_stats(self, text):\n",
    "        print(len(re.findall(NormalizerI.BYTE_ORDER_MARK, text)))\n",
    "        print(len(re.findall(NormalizerI.BYTE_ORDER_MARK_2, text)))\n",
    "        print(len(re.findall(NormalizerI.WORD_JOINER, text)))\n",
    "        print(len(re.findall(NormalizerI.SOFT_HYPHEN, text)))\n",
    "\n",
    "        print(len(re.findall(NormalizerI.ZERO_WIDTH_SPACE, text)))\n",
    "        print(len(re.findall(NormalizerI.NO_BREAK_SPACE, text)))\n",
    "\n",
    "        print(len(re.findall(NormalizerI.ZERO_WIDTH_NON_JOINER, text)))\n",
    "        print(len(re.findall(NormalizerI.ZERO_WIDTH_JOINER, text)))\n",
    "\n",
    "        # for mobj in re.finditer(NormalizerI.ZERO_WIDTH_NON_JOINER,text):\n",
    "        #    print text[mobj.start()-10:mobj.end()+10].replace('\\n', ' ').replace(NormalizerI.ZERO_WIDTH_NON_JOINER,'').encode('utf-8')\n",
    "        # print hex(ord(text[mobj.end():mobj.end()+1]))\n",
    "\n",
    "    def correct_visarga(self, text, visarga_char, char_range):\n",
    "        text = re.sub(r\"([\\u0900-\\u097f]):\", \"\\\\1\\u0903\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class DevanagariNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Devanagari script. In addition to basic normalization by the super class,\n",
    "    * Replaces the composite characters containing nuktas by their decomposed form\n",
    "    * replace pipe character '|' by poorna virama character\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    NUKTA = \"\\u093c\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"hi\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "    ):\n",
    "        super(DevanagariNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(DevanagariNormalizer, self).normalize(text)\n",
    "\n",
    "        # chandra a replacement for Marathi\n",
    "        text = text.replace(\"\\u0972\", \"\\u090f\")\n",
    "\n",
    "        # decomposing Nukta based composite characters\n",
    "        text = text.replace(\"\\u0929\", \"\\u0928\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0931\", \"\\u0930\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0934\", \"\\u0933\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0958\", \"\\u0915\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0959\", \"\\u0916\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095a\", \"\\u0917\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095b\", \"\\u091c\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095c\", \"\\u0921\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095d\", \"\\u0922\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095e\", \"\\u092b\" + DevanagariNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095f\", \"\\u092f\" + DevanagariNormalizer.NUKTA)\n",
    "\n",
    "        if self.remove_nuktas:\n",
    "            text = text.replace(DevanagariNormalizer.NUKTA, \"\")\n",
    "\n",
    "        # replace pipe character for poorna virama\n",
    "        text = text.replace(\"\\u007c\", \"\\u0964\")\n",
    "\n",
    "        # correct visarga\n",
    "        text = re.sub(r\"([\\u0900-\\u097f]):\", \"\\\\1\\u0903\", text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            # Extract parts containing digits\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"hi\"))\n",
    "\n",
    "        return text\n",
    "\n",
    "    def get_char_stats(self, text):\n",
    "        super(DevanagariNormalizer, self).get_char_stats(text)\n",
    "\n",
    "        print((len(re.findall(\"\\u0929\", text))))\n",
    "        print((len(re.findall(\"\\u0931\", text))))\n",
    "        print((len(re.findall(\"\\u0934\", text))))\n",
    "        print((len(re.findall(\"\\u0958\", text))))\n",
    "        print((len(re.findall(\"\\u0959\", text))))\n",
    "        print((len(re.findall(\"\\u095a\", text))))\n",
    "        print((len(re.findall(\"\\u095b\", text))))\n",
    "        print((len(re.findall(\"\\u095c\", text))))\n",
    "        print((len(re.findall(\"\\u095d\", text))))\n",
    "        print((len(re.findall(\"\\u095e\", text))))\n",
    "        print((len(re.findall(\"\\u095f\", text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d77710-3178-45df-ab5c-811a34c0a331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'चीर धाराएं समुद्र तट पर लगकर लौटने वाली लहरों का प्रवाह होता है अक्सर एक चट्टान या इसी तरह के पदार्थों पर'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = DevanagariNormalizer()\n",
    "TEST_RESULT = \"चीर धाराएं समुद्र तट पर लगकर लौटने वाली लहरों का प्रवाह होता है अक्सर एक चट्टान या इसी तरह के पदार्थों पर\"\n",
    "hi_text = \"चीर धाराएं समुद्र तट पर लगकर लौटने वाली लहरों का प्रवाह होता है अक्सर एक चट्टान या इसी तरह के पदार्थों पर\"\n",
    "norm(hi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert norm(hi_text) == TEST_RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e5d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'भारत का सकल घरेलू उत्पाद एक.पाँच ट्रिलियन अमेरिकी डॉलर है।'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"भारत का सकल घरेलू उत्पाद 1.5 ट्रिलियन अमेरिकी डॉलर है।\"\n",
    "norm(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d00c4d-c543-4142-ab09-2d7287e5bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class HindiNormalizer(BaseNormalizer):\n",
    "    \"\"\"Fork of Devanagiri normalizer. With additional changes for Hindi and tts_mode.\"\"\"\n",
    "\n",
    "    NUKTA = \"\\u093c\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"hi\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(HindiNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(HindiNormalizer, self).normalize(text)\n",
    "\n",
    "        # chandra a replacement for Marathi\n",
    "        text = text.replace(\"\\u0972\", \"\\u090f\")\n",
    "\n",
    "        # decomposing Nukta based composite characters\n",
    "        text = text.replace(\"\\u0929\", \"\\u0928\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0931\", \"\\u0930\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0934\", \"\\u0933\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0958\", \"\\u0915\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0959\", \"\\u0916\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095a\", \"\\u0917\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095b\", \"\\u091c\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095c\", \"\\u0921\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095d\", \"\\u0922\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095e\", \"\\u092b\" + HindiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u095f\", \"\\u092f\" + HindiNormalizer.NUKTA)\n",
    "\n",
    "        if self.remove_nuktas:\n",
    "            text = text.replace(HindiNormalizer.NUKTA, \"\")\n",
    "\n",
    "        # replace pipe character for poorna virama\n",
    "        text = text.replace(\"\\u007c\", \"\\u0964\")\n",
    "        # correct visarga\n",
    "        text = re.sub(r\"([\\u0900-\\u097f]):\", \"\\\\1\\u0903\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"रुपये \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"रुपये \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"रुपये \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"डॉलर \\1\", text)\n",
    "            text = re.sub(r\"\\$\\s*(\\d+)\", r\"डॉलर \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"कोरियाई वॉन \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"कोरियाई वॉन \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                # Convert both parts to Hindi words\n",
    "                whole_words = num2words(whole, lang=\"hi\")\n",
    "                frac_words = \" \".join(num2words(digit, lang=\"hi\") for digit in frac)\n",
    "                return f\"{whole_words} पॉइंट {frac_words}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Normalize full URLs\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"एच टी टी पी एस कोलन स्लैश स्लैश \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"एच टी टी पी कोलन स्लैश स्लैश \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" डॉट \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" स्लैश \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" स्लैश \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domains like www.amazon.in, openai.com\n",
    "            def normalize_bare_url(match):\n",
    "                domain_parts = match.group(0).split(\".\")\n",
    "                return \" डॉट \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Email normalization\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} एट {m.group(2)} डॉट {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "            # Replace standalone special characters\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "            text = text.lower()\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            # digit_parts = re.findall(r\"\\d+\", text)\n",
    "            digit_parts = re.findall(r\"\\d+\\.\\d+|\\d+\", text)\n",
    "            for x in digit_parts:\n",
    "                text = text.replace(x, num2words(x, lang=\"hi\"))\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63810f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'चीर धाराएं समुद्र तट पर लगकर लौटने वाली लहरों का प्रवाह होता है अक्सर एक चट्टान या इसी तरह के पदार्थों पर डॉलर एक सौ चौंतीस पॉइंट पाँच'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = HindiNormalizer(tts_mode=True)\n",
    "TEST_RESULT = \"चीर धाराएं समुद्र तट पर लगकर लौटने वाली लहरों का प्रवाह होता है अक्सर एक चट्टान या इसी तरह के पदार्थों पर\"\n",
    "hi_text = \"चीर धाराएं समुद्र तट पर लगकर लौटने वाली लहरों का प्रवाह होता है अक्सर एक चट्टान या इसी तरह के पदार्थों पर $134.5\"\n",
    "norm(hi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d649932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he spent रुपये पाँच सौ and डॉलर बीस on groceries.\n"
     ]
    }
   ],
   "source": [
    "normalizer = HindiNormalizer(tts_mode=True)\n",
    "output = normalizer(\"He spent Rs. 500 and $20 on groceries.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1913ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visit एच टी टी पी एस कोलन स्लैश स्लैश e x a m p l e डॉट c o m or mail us at help at s u p p o r t डॉट i n  or  get बीस percent  off!\n"
     ]
    }
   ],
   "source": [
    "normalizer = HindiNormalizer(tts_mode=True)\n",
    "output = normalizer(\n",
    "    \"Visit https://example.com or mail us at help@support.in & get 20% off!\"\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade06f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आज की तारीख में भारत में रुपये एक सौ अमेरिका में डॉलर एक पॉइंट एक आठ पाँच पाँच के बराबर है alexerws at g m a i l डॉट c o m'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"आज की तारीख में भारत में ₹100 अमेरिका में $1.1855 के बराबर है alexerws@gmail.com\"\n",
    "normalizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'प्रश्न आठ चे उत्तर सत्रह पॉइंट आठ दो पाँच आहे.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"प्रश्न 8 चे उत्तर 17.825 आहे.\"\n",
    "normalizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd215d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'भारत का सकल घरेलू उत्पाद एक पॉइंट पाँच ट्रिलियन अमेरिकी डॉलर है।'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"भारत का सकल घरेलू उत्पाद 1.5 ट्रिलियन अमेरिकी डॉलर है।\"\n",
    "normalizer(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class PunjabiNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Gurmukhi script. In addition to basic normalization by the super class,\n",
    "    * Replaces the composite characters containing nuktas by their decomposed form\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * replace pipe character '|' by poorna virama character\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    NUKTA = \"\\u0a3c\"\n",
    "\n",
    "    VOWEL_NORM_MAPS = {\n",
    "        ## http://www.unicode.org/versions/Unicode12.1.0/ch12.pdf\n",
    "        ## Table 12-16\n",
    "        \"\\u0a05\\u0a3e\": \"\\u0a06\",\n",
    "        \"\\u0a72\\u0a3f\": \"\\u0a07\",\n",
    "        \"\\u0a72\\u0a40\": \"\\u0a08\",\n",
    "        \"\\u0a73\\u0a41\": \"\\u0a09\",\n",
    "        \"\\u0a73\\u0a42\": \"\\u0a0a\",\n",
    "        \"\\u0a72\\u0a47\": \"\\u0a0f\",\n",
    "        \"\\u0a05\\u0a48\": \"\\u0a10\",\n",
    "        \"\\u0a73\\u0a4b\": \"\\u0a13\",\n",
    "        \"\\u0a05\\u0a4c\": \"\\u0a14\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"pa\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        do_canonicalize_addak=False,\n",
    "        do_canonicalize_tippi=False,\n",
    "        do_replace_vowel_bases=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(PunjabiNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.do_canonicalize_addak = do_canonicalize_addak\n",
    "        self.do_canonicalize_tippi = do_canonicalize_tippi\n",
    "        self.do_replace_vowel_bases = do_replace_vowel_bases\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def _normalize_vowels(self, text):\n",
    "        \"\"\" \"\"\"\n",
    "\n",
    "        ## standard vowel replacements as per suggestions in\n",
    "        ## http://www.unicode.org/versions/Unicode12.1.0/ch12.pdf\n",
    "        ## Table 12-16\n",
    "\n",
    "        for k, v in PunjabiNormalizer.VOWEL_NORM_MAPS.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "        ## the above mappings should account for majority of the variantions,\n",
    "        ## Rest are handled via this generic rule which looks at the diacritic\n",
    "        ## following the 2 special characters\n",
    "        ## TBD: don't see evidence for this in Wikipedia corpus\n",
    "\n",
    "        ## If these special characters occur without any diacritic, replace them with closet\n",
    "        ## equivalent vowels\n",
    "        if self.do_replace_vowel_bases:\n",
    "            text = text.replace(\"\\u0a72\", \"\\u0a07\")\n",
    "            text = text.replace(\"\\u0a73\", \"\\u0a09\")\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # Addak\n",
    "        if self.do_canonicalize_addak:\n",
    "            ## replace addak+consonant with consonat+halant+consonant\n",
    "            text = re.sub(r\"\\u0a71(.)\", \"\\1\\u0a4d\\1\", text)\n",
    "\n",
    "        # Tippi\n",
    "        if self.do_canonicalize_tippi:\n",
    "            text = text.replace(\"\\u0a70\", \"\\u0a02\")\n",
    "\n",
    "        # Vowels: Gurumuki has multiple ways of representing independent vowels due\n",
    "        # to the characters 'iri' and 'ura'.\n",
    "        text = self._normalize_vowels(text)\n",
    "\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(PunjabiNormalizer, self).normalize(text)\n",
    "\n",
    "        # decomposing Nukta based composite characters\n",
    "        text = text.replace(\"\\u0a33\", \"\\u0a32\" + PunjabiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0a36\", \"\\u0a38\" + PunjabiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0a59\", \"\\u0a16\" + PunjabiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0a5a\", \"\\u0a17\" + PunjabiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0a5b\", \"\\u0a1c\" + PunjabiNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0a5e\", \"\\u0a2b\" + PunjabiNormalizer.NUKTA)\n",
    "\n",
    "        if self.remove_nuktas:\n",
    "            text = text.replace(PunjabiNormalizer.NUKTA, \"\")\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u0a64\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u0a65\", \"\\u0965\")\n",
    "\n",
    "        ## replace pipe character for poorna virama\n",
    "        text = text.replace(\"\\u007c\", \"\\u0964\")\n",
    "\n",
    "        # correct visarge\n",
    "        text = re.sub(r\"([\\u0a00-\\u0a7f]):\", \"\\1\\u0a03\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"ਰੁਪਏ \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"ਰੁਪਏ \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"ਰੁਪਏ \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"ਡਾਲਰ \\1\", text)\n",
    "            text = re.sub(r\"$\\s*(\\d+)\", r\"ਡਾਲਰ \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"ਕੋਰੀਆਈ ਵੌਨ \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"ਕੋਰੀਆਈ ਵੌਨ \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} ਪੌਇੰਟ {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Normalize full URLs\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"ਐਚ ਟੀ ਟੀ ਪੀ ਐੱਸ ਕੋਲਨ ਸਲੈਸ਼ ਸਲੈਸ਼ \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"ਐਚ ਟੀ ਟੀ ਪੀ ਕੋਲਨ ਸਲੈਸ਼ ਸਲੈਸ਼ \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" ਡਾਟ \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" ਸਲੈਸ਼ \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" ਸਲੈਸ਼ \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domains (e.g., www.amazon.in)\n",
    "            def normalize_bare_url(match):\n",
    "                domain_parts = match.group(0).split(\".\")\n",
    "                return \" ਡਾਟ \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Email normalization\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} ਐਟ {m.group(2)} ਡਾਟ {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "\n",
    "            # Replace standalone special characters\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            # digit_parts = re.findall(r\"\\d+\", text)\n",
    "            digit_parts = re.findall(r\"\\d+\\.\\d+|\\d+\", text)\n",
    "            for x in digit_parts:\n",
    "                text = text.replace(x, num2words(x, lang=\"pa\"))\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077cb1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ਪੰਜੀਵੀ ਜੀਡੀਪੀ ਇੱਕ ਪੌਇੰਟ ਪੰਜ ਟ੍ਰੀਲੀਅਨ ਅਮਰੀਕਾ ਡੋਲਰ ਛੇ।'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = PunjabiNormalizer(tts_mode=True)\n",
    "punjabi_text = \"ਪੰਜੀਵੀ ਜੀਡੀਪੀ 1.5 ਟ੍ਰੀਲੀਅਨ ਅਮਰੀਕਾ ਡੋਲਰ ਛੇ।\"\n",
    "normalizer(punjabi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332b715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ਸਤਿ ਸ੍ਰੀ ਅਕਾਲ ਦੁਨੀਆਂ। ਚਾਰ ਮਈ, ਦੋ ਹਜ਼ਾਰ ਪੱਚੀ ਨੂੰ ਰਿਲੀਜ਼ ਹੋਏ ਪੰਜਾਬੀ ਨੋਰਮਲਾਈਜ਼ਰ ਵਿੱਚ ਤੁਹਾਡਾ ਸਵਾਗਤ ਹੈ।'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punjabi_text = (\n",
    "    \"ਸਤਿ ਸ੍ਰੀ ਅਕਾਲ ਦੁਨੀਆਂ। 4 ਮਈ, 2025 ਨੂੰ ਰਿਲੀਜ਼ ਹੋਏ ਪੰਜਾਬੀ ਨੋਰਮਲਾਈਜ਼ਰ ਵਿੱਚ ਤੁਹਾਡਾ ਸਵਾਗਤ ਹੈ।\"\n",
    ")\n",
    "normalizer(punjabi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class TeluguNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Teluguscript. In addition to basic normalization by the super class,\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * canonicalize two-part dependent vowel signs\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"te\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(TeluguNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(TeluguNormalizer, self).normalize(text)\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u0c64\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u0c65\", \"\\u0965\")\n",
    "\n",
    "        # dependent vowels\n",
    "        text = text.replace(\"\\u0c46\\u0c56\", \"\\u0c48\")\n",
    "\n",
    "        # correct visarge\n",
    "        text = re.sub(r\"([\\u0c00-\\u0c7f]):\", \"\\1\\u0c03\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"రూపాయలు \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"రూపాయలు \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"రూపాయలు \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"డాలర్లు \\1\", text)\n",
    "            text = re.sub(r\"$\\s*(\\d+)\", r\"డాలర్లు \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"కొరియన్ వాన్ \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"కొరియన్ వాన్ \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} పాయింట్ {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Normalize full URLs\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"హెచ్ టి టి పి ఎస్ కొలన్ స్లాష్ స్లాష్ \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"హెచ్ టి టి పి కొలన్ స్లాష్ స్లాష్ \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" డాట్ \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" స్లాష్ \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" స్లాష్ \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domain names\n",
    "            def normalize_bare_url(match):\n",
    "                domain_parts = match.group(0).split(\".\")\n",
    "                return \" డాట్ \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Email normalization\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} అట్ {m.group(2)} డాట్ {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "            # Replace standalone special characters\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"te\"))\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'భారతదేశ జీడీపీ ఒకటి.ఐదు ట్రిలియన్ అమెరికా డాలర్లు.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = TeluguNormalizer()\n",
    "te_text = \"భారతదేశ జీడీపీ 1.5 ట్రిలియన్ అమెరికా డాలర్లు.\"\n",
    "norm(te_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917bff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'భారతదేశ జీడీపీ ఒకటి పాయింట్ ఐదు ట్రిలియన్ అమెరికా డాలర్లు.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = TeluguNormalizer(tts_mode=True)\n",
    "norm(te_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b564a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class GujaratiNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Gujarati script. In addition to basic normalization by the super class,\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    NUKTA = \"\\u0abc\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"gu\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(GujaratiNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(GujaratiNormalizer, self).normalize(text)\n",
    "\n",
    "        # decomposing Nukta based composite characters\n",
    "        if self.remove_nuktas:\n",
    "            text = text.replace(GujaratiNormalizer.NUKTA, \"\")\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u0ae4\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u0ae5\", \"\\u0965\")\n",
    "\n",
    "        # correct visarge\n",
    "        text = re.sub(r\"([\\u0a80-\\u0aff]):\", \"\\\\1\\u0a83\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"રૂપિયા \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"રૂપિયા \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"રૂપિયા \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"ડોલર \\1\", text)\n",
    "            text = re.sub(r\"\\$\\s*(\\d+)\", r\"ડોલર \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"કોરિયન વોન \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"કોરિયન વોન \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} પોઈન્ટ {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Normalize full URLs\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"એચ ટી ટી પીએસ કોલન સ્લેશ સ્લેશ \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"એચ ટી ટી પી કોલન સ્લેશ સ્લેશ \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" ડોટ \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" સ્લેશ \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" સ્લેશ \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domain names\n",
    "            def normalize_bare_url(match):\n",
    "                domain_parts = match.group(0).split(\".\")\n",
    "                return \" ડોટ \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Normalize email addresses\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} એટ {m.group(2)} ડોટ {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"gu\"))\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3c434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ભારતનો જીડીપી એક પોઈન્ટ પાંચ ટ્રિલિયન અમેરિકન ડોલર છે.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = GujaratiNormalizer(tts_mode=True)\n",
    "gujarati_text = \"ભારતનો જીડીપી 1.5 ટ્રિલિયન અમેરિકન ડોલર છે.\"\n",
    "norm(gujarati_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class OdiaNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Oriya script. In addition to basic normalization by the super class,\n",
    "    * Replaces the composite characters containing nuktas by their decomposed form\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * Canonicalize two part dependent vowels\n",
    "    * Replace 'va' with 'ba'\n",
    "    * replace pipe character '|' by poorna virama character\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    NUKTA = \"\\u0b3c\"\n",
    "\n",
    "    VOWEL_NORM_MAPS = {\n",
    "        ## See Table 12-22 in http://www.unicode.org/versions/Unicode12.1.0/ch12.pdf\n",
    "        \"\\u0b05\\u0b3e\": \"\\u0b06\",\n",
    "        \"\\u0b0f\\u0b57\": \"\\u0b10\",\n",
    "        \"\\u0b13\\u0b57\": \"\\u0b14\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"or\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        do_remap_wa=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(OdiaNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.do_remap_wa = do_remap_wa\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(OdiaNormalizer, self).normalize(text)\n",
    "\n",
    "        ## standard vowel replacements as per suggestions in Unicode documents\n",
    "        for k, v in OdiaNormalizer.VOWEL_NORM_MAPS.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "        # decomposing Nukta based composite characters\n",
    "        text = text.replace(\"\\u0b5c\", \"\\u0b21\" + OdiaNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u0b5d\", \"\\u0b22\" + OdiaNormalizer.NUKTA)\n",
    "\n",
    "        if self.remove_nuktas:\n",
    "            text = text.replace(OdiaNormalizer.NUKTA, \"\")\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u0b64\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u0b65\", \"\\u0965\")\n",
    "\n",
    "        # replace pipe character for poorna virama\n",
    "        text = text.replace(\"\\u0b7c\", \"\\u0964\")\n",
    "\n",
    "        # replace wa with ba\n",
    "        if self.do_remap_wa:\n",
    "            text = text.replace(\"\\u0b71\", \"\\u0b2c\")\n",
    "\n",
    "        # replace va with ba\n",
    "        # NOTE: documentation (chapter on Indic scripts) and codepoint chart seem contradictory\n",
    "        # (this applied to wa to ba rule also above)\n",
    "        text = text.replace(\"\\u0b35\", \"\\u0b2c\")\n",
    "\n",
    "        # AI dependent vowel sign\n",
    "        text = text.replace(\"\\u0b47\\u0b56\", \"\\u0b58\")\n",
    "\n",
    "        # two part dependent vowels\n",
    "        text = text.replace(\"\\u0b47\\u0b3e\", \"\\u0b4b\")\n",
    "        text = text.replace(\"\\u0b47\\u0b57\", \"\\u0b4c\")\n",
    "\n",
    "        # additional consonant - not clear how to handle this\n",
    "        # ignore\n",
    "\n",
    "        # correct visarge\n",
    "        text = re.sub(r\"([\\u0b00-\\u0b7f]):\", \"\\\\1\\u0b03\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"ଟଙ୍କା \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"ଟଙ୍କା \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"ଟଙ୍କା \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"ଡଲାର \\1\", text)\n",
    "            text = re.sub(r\"\\$\\s*(\\d+)\", r\"ଡଲାର \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"କୋରିଆନ୍ ୱନ୍ \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"କୋରିଆନ୍ ୱନ୍ \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} ପଏଣ୍ଟ {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"ଏଚ୍ ଟି ଟି ପି ଏସ୍ କୋଲନ୍ ସ୍ଲାଶ୍ ସ୍ଲାଶ୍ \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"ଏଚ୍ ଟି ଟି ପି କୋଲନ୍ ସ୍ଲାଶ୍ ସ୍ଲାଶ୍ \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" ଡଟ୍ \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" ସ୍ଲାଶ୍ \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" ସ୍ଲାଶ୍ \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domains like www.amazon.in, openai.com\n",
    "            def normalize_bare_url(match):\n",
    "                domain_parts = match.group(0).split(\".\")\n",
    "                return \" ଡଟ୍ \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Email normalization\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} ଏଟ୍ {m.group(2)} ଡଟ୍ {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"or\"))\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a1249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ଭାରତର ଜିଡିପି ଏକ ପଏଣ୍ଟ ପାଞ୍ଚ ଟ୍ରିଲିଅନ୍ ଆମେରିକୀୟ ଡଲାର ଅଟେ।'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = OdiaNormalizer(tts_mode=True)\n",
    "odia_text = \"ଭାରତର ଜିଡିପି 1.5 ଟ୍ରିଲିଅନ୍ ଆମେରିକୀୟ ଡଲାର ଅଟେ।\"\n",
    "norm(odia_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BengaliNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Bengali script. In addition to basic normalization by the super class,\n",
    "    * Replaces the composite characters containing nuktas by their decomposed form\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * Canonicalize two part dependent vowels\n",
    "    * replace pipe character '|' by poorna virama character\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    NUKTA = \"\\u09bc\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"bn\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        do_remap_assamese_chars=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(BengaliNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.do_remap_assamese_chars = do_remap_assamese_chars\n",
    "        self.tts_mode = True\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(BengaliNormalizer, self).normalize(text)\n",
    "\n",
    "        # decomposing Nukta based composite characters\n",
    "        text = text.replace(\"\\u09dc\", \"\\u09a1\" + BengaliNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u09dd\", \"\\u09a2\" + BengaliNormalizer.NUKTA)\n",
    "        text = text.replace(\"\\u09df\", \"\\u09af\" + BengaliNormalizer.NUKTA)\n",
    "\n",
    "        if self.remove_nuktas:\n",
    "            text = text.replace(BengaliNormalizer.NUKTA, \"\")\n",
    "\n",
    "        if self.do_remap_assamese_chars and self.lang == \"as\":\n",
    "            text = text.replace(\"\\u09f0\", \"\\u09b0\")  #  'ra' character\n",
    "            text = text.replace(\"\\u09f1\", \"\\u09ac\")  #  'va' character\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u09e4\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u09e5\", \"\\u0965\")\n",
    "\n",
    "        # replace pipe character for poorna virama\n",
    "        text = text.replace(\"\\u007c\", \"\\u0964\")\n",
    "        # replace bengali currency numerator four for poorna virama  (it looks similar and is used as a substitute)\n",
    "        text = text.replace(\"\\u09f7\", \"\\u0964\")\n",
    "\n",
    "        # two part dependent vowels\n",
    "        text = text.replace(\"\\u09c7\\u09be\", \"\\u09cb\")\n",
    "        text = text.replace(\"\\u09c7\\u09d7\", \"\\u09cc\")\n",
    "\n",
    "        # correct visarge\n",
    "        text = re.sub(r\"([\\u0980-\\u09ff]):\", \"\\\\1\\u0983\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"রুপি \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"রুপি \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"রুপি \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"ডলার \\1\", text)\n",
    "            text = re.sub(r\"\\$\\s*(\\d+)\", r\"ডলার \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"কোরিয়ান ওন \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"কোরিয়ান ওন \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} পয়েন্ট {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Full URL normalization\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"এইচ টি টি পি এস কোলন স্ল্যাশ স্ল্যাশ \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"এইচ টি টি পি কোলন স্ল্যাশ স্ল্যাশ \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" ডট \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" স্ল্যাশ \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" স্ল্যাশ \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Bare domain normalization\n",
    "            def normalize_bare_url(match):\n",
    "                domain_parts = match.group(0).split(\".\")\n",
    "                return \" ডট \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Emails\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} অ্যাট {m.group(2)} ডট {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "            # Replace standalone special characters\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"bn\"))\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a21ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ভারতের জিডিপি এক পয়েন্ট পাঁচ ট্রিলিয়ন মার্কিন ডলার।'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = BengaliNormalizer(tts_mode=True)\n",
    "\n",
    "TEST_RESULT = \"ভারতের জিডিপি ১.৫ ট্রিলিয়ন মার্কিন ডলার।\"  # Claude generated output\n",
    "bn_text = \"ভারতের জিডিপি 1.5 ট্রিলিয়ন মার্কিন ডলার।\"\n",
    "norm(bn_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86932630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class TamilNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Tamil script. In addition to basic normalization by the super class,\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * canonicalize two-part dependent vowel signs\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"ta\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(TamilNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(TamilNormalizer, self).normalize(text)\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u0be4\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u0be5\", \"\\u0965\")\n",
    "\n",
    "        # two part dependent vowels\n",
    "        text = text.replace(\"\\u0b92\\u0bd7\", \"\\u0b94\")\n",
    "        text = text.replace(\"\\u0bc6\\u0bbe\", \"\\u0bca\")\n",
    "        text = text.replace(\"\\u0bc7\\u0bbe\", \"\\u0bcb\")\n",
    "        text = text.replace(\"\\u0bc6\\u0bd7\", \"\\u0bcc\")\n",
    "\n",
    "        # correct visarge\n",
    "        text = re.sub(r\"([\\u0b80-\\u0bff]):\", \"\\\\1\\u0b83\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"ரூபாய் \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"ரூபாய் \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"ரூபாய் \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"டாலர் \\1\", text)\n",
    "            text = re.sub(r\"\\$\\s*(\\d+)\", r\"டாலர் \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"கொரிய வான் \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"கொரிய வான் \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} பாயிண்ட் {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Normalize full URLs\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"எச் டி டி பி எஸ் கோலன் ஸ்லாஷ் ஸ்லாஷ் \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"எச் டி டி பி கோலன் ஸ்லாஷ் ஸ்லாஷ் \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" டாட் \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" ஸ்லாஷ் \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" ஸ்லாஷ் \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domains (www.amazon.in)\n",
    "            def normalize_bare_url(match):\n",
    "                domain_parts = match.group(0).split(\".\")\n",
    "                return \" டாட் \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Emails\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} அட் {m.group(2)} டாட் {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "\n",
    "            # Replace standalone special characters\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"ta\"))\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de91fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'இந்தியாவின் மொத்த உள்நாட்டு உற்பத்தி ஒன்று பாயிண்ட் ஐந்து டிரில்லியன் அமெரிக்க டாலர்.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = TamilNormalizer(tts_mode=True)\n",
    "ta_text = \"இந்தியாவின் மொத்த உள்நாட்டு உற்பத்தி 1.5 டிரில்லியன் அமெரிக்க டாலர்.\"\n",
    "norm(ta_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f64e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'அவர் ரூபாய் ஐநூறு மற்றும் டாலர் இருபது க்கு உணவுப்பொருட்கள் வாங்கினார். இணையதளம்ஃ w w w டாட் a m a z o n டாட் i n.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = TamilNormalizer(tts_mode=True)\n",
    "ta_text = \"அவர் Rs. 500 மற்றும் $20 க்கு உணவுப்பொருட்கள் வாங்கினார். இணையதளம்: www.amazon.in.\"\n",
    "norm(ta_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99dc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class KannadaNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Kannada script. In addition to basic normalization by the super class,\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * canonicalize two-part dependent vowel signs\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"kn\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(KannadaNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(KannadaNormalizer, self).normalize(text)\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u0ce4\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u0ce5\", \"\\u0965\")\n",
    "\n",
    "        # dependent vowels\n",
    "        text = text.replace(\"\\u0cbf\\u0cd5\", \"\\u0cc0\")\n",
    "        text = text.replace(\"\\u0cc6\\u0cd5\", \"\\u0cc7\")\n",
    "        text = text.replace(\"\\u0cc6\\u0cd6\", \"\\u0cc8\")\n",
    "        text = text.replace(\"\\u0cc6\\u0cc2\", \"\\u0cca\")\n",
    "        text = text.replace(\"\\u0cca\\u0cd5\", \"\\u0ccb\")\n",
    "\n",
    "        # correct visarge\n",
    "        text = re.sub(r\"([\\u0c80-\\u0cff]):\", \"\\\\1\\u0c83\", text)\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"ರೂಪಾಯಿ \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"ರೂಪಾಯಿ \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"ರೂಪಾಯಿ \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"ಡಾಲರ್ \\1\", text)\n",
    "            text = re.sub(r\"\\$\\s*(\\d+)\", r\"ಡಾಲರ್ \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"ಕೊರಿಯನ್ ವಾನ್ \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"ಕೊರಿಯನ್ ವಾನ್ \\1\", text)\n",
    "\n",
    "            # Handle decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} ಪಾಯಿಂಟ್ {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Normalize full URLs\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"ಎಚ್ ಟಿ ಟಿ ಪಿ ಎಸ್ ಕೋಲನ್ ಸ್ಲ್ಯಾಶ್ ಸ್ಲ್ಯಾಶ್ \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"ಎಚ್ ಟಿ ಟಿ ಪಿ ಕೋಲನ್ ಸ್ಲ್ಯಾಶ್ ಸ್ಲ್ಯಾಶ್ \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" ಡಾಟ್ \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" ಸ್ಲ್ಯಾಶ್ \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" ಸ್ಲ್ಯಾಶ್ \" + spoken_path if spoken_path else \"\"\n",
    "\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domains like www.amazon.in\n",
    "            def normalize_bare_url(match):\n",
    "                url = match.group(0)\n",
    "                domain_parts = url.split(\".\")\n",
    "                spoken = \" ಡಾಟ್ \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "                return spoken\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Normalize email\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} ಅಟ್ {m.group(2)} ಡಾಟ್ {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "\n",
    "            # Replace standalone special characters\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"kn\"))\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2462b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ಭಾರತದ ಜಿಡಿಪಿ ಒಂದು ಪಾಯಿಂಟ್ ಐದು ಟ್ರಿಲಿಯನ್ ಅಮೇರಿಕನ್ ಡಾಲರ್ ಆಗಿದೆ.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = KannadaNormalizer(tts_mode=True)\n",
    "kannada_text = \"ಭಾರತದ ಜಿಡಿಪಿ 1.5 ಟ್ರಿಲಿಯನ್ ಅಮೇರಿಕನ್ ಡಾಲರ್ ಆಗಿದೆ.\"\n",
    "norm(kannada_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcb87e-1a24-4d42-bece-d35b52d9320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MalayalamNormalizer(BaseNormalizer):\n",
    "    \"\"\"\n",
    "    Normalizer for the Malayalam script. In addition to basic normalization by the super class,\n",
    "    * Replace the reserved character for poorna virama (if used) with the recommended generic Indic scripts poorna virama\n",
    "    * canonicalize two-part dependent vowel signs\n",
    "    * Change from old encoding of chillus (till Unicode 5.0) to new encoding\n",
    "    * replace colon ':' by visarga if the colon follows a charcter in this script\n",
    "    \"\"\"\n",
    "\n",
    "    CHILLU_CHAR_MAP = {\n",
    "        \"\\u0d7a\": \"\\u0d23\",\n",
    "        \"\\u0d7b\": \"\\u0d28\",\n",
    "        \"\\u0d7c\": \"\\u0d30\",\n",
    "        \"\\u0d7d\": \"\\u0d32\",\n",
    "        \"\\u0d7e\": \"\\u0d33\",\n",
    "        \"\\u0d7f\": \"\\u0d15\",\n",
    "    }\n",
    "\n",
    "    def _canonicalize_chillus(self, text):\n",
    "        for chillu, char in MalayalamNormalizer.CHILLU_CHAR_MAP.items():\n",
    "            text = text.replace(chillu, \"{}\\u0d4d\".format(char))\n",
    "        return text\n",
    "\n",
    "    def _correct_geminated_T(self, text):\n",
    "        return text.replace(\"\\u0d31\\u0d4d\\u0d31\", \"\\u0d1f\\u0d4d\\u0d1f\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang=\"ml\",\n",
    "        remove_nuktas=False,\n",
    "        nasals_mode=\"do_nothing\",\n",
    "        do_normalize_chandras=False,\n",
    "        do_normalize_vowel_ending=False,\n",
    "        do_canonicalize_chillus=False,\n",
    "        do_correct_geminated_T=False,\n",
    "        tts_mode=False,\n",
    "    ):\n",
    "        super(MalayalamNormalizer, self).__init__(\n",
    "            lang,\n",
    "            remove_nuktas,\n",
    "            nasals_mode,\n",
    "            do_normalize_chandras,\n",
    "            do_normalize_vowel_ending,\n",
    "        )\n",
    "        self.do_canonicalize_chillus = do_canonicalize_chillus\n",
    "        self.do_correct_geminated_T = do_correct_geminated_T\n",
    "        self.tts_mode = tts_mode\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # Change from old encoding of chillus (till Unicode 5.0) to new encoding\n",
    "        text = text.replace(\"\\u0d23\\u0d4d\\u200d\", \"\\u0d7a\")\n",
    "        text = text.replace(\"\\u0d28\\u0d4d\\u200d\", \"\\u0d7b\")\n",
    "        text = text.replace(\"\\u0d30\\u0d4d\\u200d\", \"\\u0d7c\")\n",
    "        text = text.replace(\"\\u0d32\\u0d4d\\u200d\", \"\\u0d7d\")\n",
    "        text = text.replace(\"\\u0d33\\u0d4d\\u200d\", \"\\u0d7e\")\n",
    "        text = text.replace(\"\\u0d15\\u0d4d\\u200d\", \"\\u0d7f\")\n",
    "\n",
    "        # Normalize chillus\n",
    "        if self.do_canonicalize_chillus:\n",
    "            text = self._canonicalize_chillus(text)\n",
    "\n",
    "        # common normalization for Indic scripts\n",
    "        text = super(MalayalamNormalizer, self).normalize(text)\n",
    "\n",
    "        # replace the poorna virama codes specific to script\n",
    "        # with generic Indic script codes\n",
    "        text = text.replace(\"\\u0d64\", \"\\u0964\")\n",
    "        text = text.replace(\"\\u0d65\", \"\\u0965\")\n",
    "\n",
    "        # dependent vowels\n",
    "        text = text.replace(\"\\u0d46\\u0d3e\", \"\\u0d4a\")\n",
    "        text = text.replace(\"\\u0d47\\u0d3e\", \"\\u0d4b\")\n",
    "\n",
    "        # au forms\n",
    "        text = text.replace(\"\\u0d46\\u0d57\", \"\\u0d4c\")\n",
    "        text = text.replace(\"\\u0d57\", \"\\u0d4c\")\n",
    "\n",
    "        # correct geminated T\n",
    "        if self.do_correct_geminated_T:\n",
    "            text = self._correct_geminated_T(text)\n",
    "\n",
    "        # correct visarga\n",
    "        text = re.sub(r\"([\\u0d00-\\u0d7f]):\", \"\\\\1\\u0d03\", text)\n",
    "        # remove samvruthokaram\n",
    "        text = text.replace(\"\\u0d41\\u0d4d\", \"\\u0d4d\")\n",
    "\n",
    "        if self.tts_mode:\n",
    "            # Handle currencies\n",
    "            text = re.sub(r\"INR\\s+(\\d+)\", r\"രൂപ \\1\", text)\n",
    "            text = re.sub(r\"Rs\\.\\s+(\\d+)\", r\"രൂപ \\1\", text)\n",
    "            text = re.sub(r\"₹\\s*(\\d+)\", r\"രൂപ \\1\", text)\n",
    "            text = re.sub(r\"USD\\s+(\\d+)\", r\"ഡോളർ \\1\", text)\n",
    "            text = re.sub(r\"\\$\\s*(\\d+)\", r\"ഡോളർ \\1\", text)\n",
    "            text = re.sub(r\"KRW\\s+(\\d+)\", r\"കൊറിയൻ വോൺ \\1\", text)\n",
    "            text = re.sub(r\"₩\\s*(\\d+)\", r\"കൊറിയൻ വോൺ \\1\", text)\n",
    "\n",
    "            # Decimal numbers\n",
    "            def replace_decimal(match):\n",
    "                whole, frac = match.group(1), match.group(2)\n",
    "                return f\"{whole} പോയിന്റ് {frac}\"\n",
    "\n",
    "            text = re.sub(r\"(\\d+)\\.(\\d+)\", replace_decimal, text)\n",
    "\n",
    "            # Normalize full URLs (http/https)\n",
    "            def normalize_url(match):\n",
    "                url = match.group(0)\n",
    "                scheme = \"\"\n",
    "                if url.startswith(\"https://\"):\n",
    "                    scheme = \"എച്ച് ടി ടി പി എസ് കോളൺ സ്ലാഷ് സ്ലാഷ് \"\n",
    "                    url = url[len(\"https://\") :]\n",
    "                elif url.startswith(\"http://\"):\n",
    "                    scheme = \"എച്ച് ടി ടി പി കോളൺ സ്ലാഷ് സ്ലാഷ് \"\n",
    "                    url = url[len(\"http://\") :]\n",
    "                parts = url.split(\"/\", 1)\n",
    "                domain = parts[0]\n",
    "                path = parts[1] if len(parts) > 1 else \"\"\n",
    "                domain_parts = domain.split(\".\")\n",
    "                spoken_domain = \" ഡോട്ട് \".join(\n",
    "                    \" \".join(part.upper()) for part in domain_parts\n",
    "                )\n",
    "                spoken_path = \"\"\n",
    "                if path:\n",
    "                    path_parts = path.split(\"/\")\n",
    "                    spoken_path = \" സ്ലാഷ് \".join(\n",
    "                        \" \".join(p.upper()) for p in path_parts if p\n",
    "                    )\n",
    "                    spoken_path = \" സ്ലാഷ് \" + spoken_path if spoken_path else \"\"\n",
    "                return f\"{scheme}{spoken_domain}{spoken_path}\".strip()\n",
    "\n",
    "            text = re.sub(r\"https?://[^\\s]+\", normalize_url, text)\n",
    "\n",
    "            # Normalize bare domain URLs (like www.amazon.in)\n",
    "            def normalize_bare_url(match):\n",
    "                url = match.group(0)\n",
    "                domain_parts = url.split(\".\")\n",
    "                spoken = \" ഡോട്ട് \".join(\" \".join(part.upper()) for part in domain_parts)\n",
    "                return spoken\n",
    "\n",
    "            bare_url_tld_pattern = r\"\\b(?:www\\.)?[\\w-]+\\.(?:com|in|org|net|edu|gov|ai|co|io|info|biz|nic\\.in|ac\\.in|gov\\.in)\\b\"\n",
    "            text = re.sub(\n",
    "                bare_url_tld_pattern, normalize_bare_url, text, flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            # Normalize emails\n",
    "            text = re.sub(\n",
    "                r\"\\b([\\w\\.-]+)@([\\w\\.-]+)\\.(\\w+)\\b\",\n",
    "                lambda m: f\"{m.group(1)} അറ്റ് {m.group(2)} ഡോട്ട് {m.group(3)}\",\n",
    "                text,\n",
    "            )\n",
    "\n",
    "            # Replace special characters\n",
    "            symbol_map = {\n",
    "                \"&\": \" or \",\n",
    "                \"@\": \" at \",\n",
    "                \"%\": \" percent \",\n",
    "                \"+\": \" plus \",\n",
    "                \"-\": \" minus \",\n",
    "                \"=\": \" equals \",\n",
    "                \"*\": \" star \",\n",
    "                \"#\": \" hash \",\n",
    "            }\n",
    "            for sym, word in symbol_map.items():\n",
    "                text = text.replace(sym, word)\n",
    "\n",
    "            def expand_acronyms(match):\n",
    "                return \" \".join(match.group(0))\n",
    "\n",
    "            text = re.sub(r\"\\b[A-Z]{2,}\\b\", expand_acronyms, text)\n",
    "\n",
    "        has_digits = any(char.isdigit() for char in text)\n",
    "        if has_digits:\n",
    "            digit_parts = re.findall(r\"\\d+\", text)\n",
    "            for part in digit_parts:\n",
    "                text = text.replace(part, num2words(part, lang=\"ml\"))\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42251bf4-298b-49ef-8a17-2bc63c590b53",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e4a2a-8a3c-45f9-b5ab-665efe1df5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MalayalamNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4425f1-a510-4af0-8b19-6127bf3e281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RESULT = \"എന്റെ കമ്പ്യൂട്ടറിന് എന്റെ ഭാഷ.\"\n",
    "text_result = normalizer(\"എന്റെ കമ്പ്യൂട്ടറിനു് എന്റെ ഭാഷ.\")\n",
    "\n",
    "assert text_result == TEST_RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290e0cd-5d59-4a2b-ae4b-f7af8e0c6aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'യുപിഎ ഭരണകാലത്തെ സാമ്പത്തിക വീഴ്ച; ധവളപത്രം ഇറക്കാൻ കേന്ദ്രസർക്കാർ.\\n\\nയുപിഎ സർക്കാരിന്റെ കാലത്തെ ധനവിനിയോഗത്തിലെ വീഴ്ചകൾ വ്യക്തമാക്കുന്ന ധവളപത്രം ഇറക്കാൻ കേന്ദ്രസർക്കാർ തീരുമാനം. ബജറ്റ് സമ്മേളനം ഇതിനായി ഒരു ദിവസം കൂടി നീട്ടും. വിഹിതങ്ങൾ എപ്രകാരം തെറ്റായി വിനിയോഗിക്കപ്പെട്ടു എന്നതുൾപ്പെടെയുള്ള കാര്യങ്ങൾ വിശദീകരിക്കാനാണ് കേന്ദ്രസർക്കാർ നീക്കം.\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTCASE_RESULT = \"യുപിഎ ഭരണകാലത്തെ സാമ്പത്തിക വീഴ്ച; ധവളപത്രം ഇറക്കാൻ കേന്ദ്രസർക്കാർ.\\n\\nയുപിഎ സർക്കാരിന്റെ കാലത്തെ ധനവിനിയോഗത്തിലെ വീഴ്ചകൾ വ്യക്തമാക്കുന്ന ധവളപത്രം ഇറക്കാൻ കേന്ദ്രസർക്കാർ തീരുമാനം. ബജറ്റ് സമ്മേളനം ഇതിനായി ഒരു ദിവസം കൂടി നീട്ടും. വിഹിതങ്ങൾ എപ്രകാരം തെറ്റായി വിനിയോഗിക്കപ്പെട്ടു എന്നതുൾപ്പെടെയുള്ള കാര്യങ്ങൾ വിശദീകരിക്കാനാണ് കേന്ദ്രസർക്കാർ നീക്കം.\\n\\n\"\n",
    "text_result = normalizer(\n",
    "    \"\"\"യുപിഎ ഭരണകാലത്തെ സാമ്പത്തിക വീഴ്ച; ധവളപത്രം ഇറക്കാൻ കേന്ദ്രസർക്കാർ.\n",
    "\n",
    "യുപിഎ സർക്കാരിന്റെ കാലത്തെ ധനവിനിയോഗത്തിലെ വീഴ്ചകൾ വ്യക്തമാക്കുന്ന ധവളപത്രം ഇറക്കാൻ കേന്ദ്രസർക്കാർ തീരുമാനം. ബജറ്റ് സമ്മേളനം ഇതിനായി ഒരു ദിവസം കൂടി നീട്ടും. വിഹിതങ്ങൾ എപ്രകാരം തെറ്റായി വിനിയോഗിക്കപ്പെട്ടു എന്നതുൾപ്പെടെയുള്ള കാര്യങ്ങൾ വിശദീകരിക്കാനാണ് കേന്ദ്രസർക്കാർ നീക്കം.\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e82960",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert text_result == TESTCASE_RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2be537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ഇന്ത്യയുടെ ജിഡിപി ഒന്ന് പോയിന്റ് അഞ്ച് ട്രില്യൺ യുഎസ് ഡോളറാണ്.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = MalayalamNormalizer(tts_mode=True)\n",
    "malaylam_text = \"ഇന്ത്യയുടെ ജിഡിപി 1.5 ട്രില്യൺ യുഎസ് ഡോളറാണ്.\"\n",
    "TESTCASE_RESULT = \"ഇന്ത്യയുടെ ജിഡിപി ഒന്ന് പോയിന്റ് അഞ്ച് ട്രില്യൺ യുഎസ് ഡോളറാണ്.\"\n",
    "text_result = normalizer(malaylam_text)\n",
    "text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e66c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert text_result == TESTCASE_RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8377c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ദുഃഖം'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTCASE_RESULT = \"ദുഃഖം\"\n",
    "text_result = normalizer(\"ദു:ഖം\")\n",
    "text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert text_result == TESTCASE_RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31a21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'എൻറെ'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTCASE_RESULT = \"എന്റെ\"\n",
    "text_result = normalizer(\"എൻറെ\")\n",
    "text_result\n",
    "\n",
    "# Still fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc0f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ആയിരം രൂപ കൊടുത്തു. അയാൾ അഞ്ഞൂറ് ഡോളറിനും ഇരുപത് ഡോളറിനും പലചരക്ക് സാധനങ്ങൾ വാങ്ങി. വെബ്സൈറ്റ്ഃ w w w ഡോട്ട് a m a z o n ഡോട്ട് i n.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer(\n",
    "    \"1000 രൂപ കൊടുത്തു. അയാൾ 500 ഡോളറിനും 20 ഡോളറിനും പലചരക്ക് സാധനങ്ങൾ വാങ്ങി. വെബ്സൈറ്റ്: www.amazon.in.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTCASE_RESULT  = \"കാണ്മാനില്ല\"\n",
    "# text_result = normalizer(\"കാണ്മാനില്ല\")\n",
    "# text_result\n",
    "\n",
    "# Still fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf881f-1182-4539-8b0e-fa6dc131cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:19: SyntaxWarning: invalid escape sequence '\\X'\n",
      "<unknown>:19: SyntaxWarning: invalid escape sequence '\\X'\n",
      "<unknown>:19: SyntaxWarning: invalid escape sequence '\\X'\n",
      "<unknown>:19: SyntaxWarning: invalid escape sequence '\\X'\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753b770-8002-455c-8b27-3be0648095ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
