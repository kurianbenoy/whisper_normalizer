{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperNormalizer English Module\n",
    "\n",
    "> OpenAI's English text standardisation module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this module do?\n",
    "\n",
    "As per the text normalization/standardization approach  Appendix Section C pp.21 the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `EnglishTextNormalizer` does the following functionality:\n",
    "\n",
    "1. Remove any phrases between matching brackets ([, ]).\n",
    "2. Remove any phrases between matching parentheses ((, )).\n",
    "3. Remove any of the following words: hmm, mm, mhm, mmm, uh, um\n",
    "4. Remove whitespace characters that comes before an apostrophe ’\n",
    "5. Convert standard or informal contracted forms of English into the original form.\n",
    "6. Remove commas (,) between digits\n",
    "7. Remove periods (.) not followed by numbers\n",
    "8. Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category\n",
    "starting with M, S, or P, except period, percent, and currency symbols that may be detected in the next step.\n",
    "9. Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. “Ten\n",
    "thousand dollars” → “$10000”.\n",
    "10. Convert British spellings into American spellings.\n",
    "11. Remove remaining symbols that are not part of any numeric expressions.\n",
    "12. Replace any successive whitespace characters with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# This code is from OpenAI Whisper Repository: https://github.com/openai/whisper/tree/main/whisper/normalizers\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from fractions import Fraction\n",
    "from typing import Iterator, List, Match, Optional, Union\n",
    "\n",
    "from importlib.resources import files\n",
    "from more_itertools import windowed\n",
    "\n",
    "from whisper_normalizer.basic import remove_symbols_and_diacritics\n",
    "\n",
    "\n",
    "class EnglishNumberNormalizer:\n",
    "    \"\"\"\n",
    "    Convert any spelled-out numbers into arabic numbers, while handling:\n",
    "\n",
    "    - remove any commas\n",
    "    - keep the suffixes such as: `1960s`, `274th`, `32nd`, etc.\n",
    "    - spell out currency symbols after the number. e.g. `$20 million` -> `20000000 dollars`\n",
    "    - spell out `one` and `ones`\n",
    "    - interpret successive single-digit numbers as nominal: `one oh one` -> `101`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.zeros = {\"o\", \"oh\", \"zero\"}\n",
    "        self.ones = {\n",
    "            name: i\n",
    "            for i, name in enumerate(\n",
    "                [\n",
    "                    \"one\",\n",
    "                    \"two\",\n",
    "                    \"three\",\n",
    "                    \"four\",\n",
    "                    \"five\",\n",
    "                    \"six\",\n",
    "                    \"seven\",\n",
    "                    \"eight\",\n",
    "                    \"nine\",\n",
    "                    \"ten\",\n",
    "                    \"eleven\",\n",
    "                    \"twelve\",\n",
    "                    \"thirteen\",\n",
    "                    \"fourteen\",\n",
    "                    \"fifteen\",\n",
    "                    \"sixteen\",\n",
    "                    \"seventeen\",\n",
    "                    \"eighteen\",\n",
    "                    \"nineteen\",\n",
    "                ],\n",
    "                start=1,\n",
    "            )\n",
    "        }\n",
    "        self.ones_plural = {\n",
    "            \"sixes\" if name == \"six\" else name + \"s\": (value, \"s\")\n",
    "            for name, value in self.ones.items()\n",
    "        }\n",
    "        self.ones_ordinal = {\n",
    "            \"zeroth\": (0, \"th\"),\n",
    "            \"first\": (1, \"st\"),\n",
    "            \"second\": (2, \"nd\"),\n",
    "            \"third\": (3, \"rd\"),\n",
    "            \"fifth\": (5, \"th\"),\n",
    "            \"twelfth\": (12, \"th\"),\n",
    "            **{\n",
    "                name + (\"h\" if name.endswith(\"t\") else \"th\"): (value, \"th\")\n",
    "                for name, value in self.ones.items()\n",
    "                if value > 3 and value != 5 and value != 12\n",
    "            },\n",
    "        }\n",
    "        self.ones_suffixed = {**self.ones_plural, **self.ones_ordinal}\n",
    "\n",
    "        self.tens = {\n",
    "            \"twenty\": 20,\n",
    "            \"thirty\": 30,\n",
    "            \"forty\": 40,\n",
    "            \"fifty\": 50,\n",
    "            \"sixty\": 60,\n",
    "            \"seventy\": 70,\n",
    "            \"eighty\": 80,\n",
    "            \"ninety\": 90,\n",
    "        }\n",
    "        self.tens_plural = {\n",
    "            name.replace(\"y\", \"ies\"): (value, \"s\") for name, value in self.tens.items()\n",
    "        }\n",
    "        self.tens_ordinal = {\n",
    "            name.replace(\"y\", \"ieth\"): (value, \"th\")\n",
    "            for name, value in self.tens.items()\n",
    "        }\n",
    "        self.tens_suffixed = {**self.tens_plural, **self.tens_ordinal}\n",
    "\n",
    "        self.multipliers = {\n",
    "            \"hundred\": 100,\n",
    "            \"thousand\": 1_000,\n",
    "            \"million\": 1_000_000,\n",
    "            \"billion\": 1_000_000_000,\n",
    "            \"trillion\": 1_000_000_000_000,\n",
    "            \"quadrillion\": 1_000_000_000_000_000,\n",
    "            \"quintillion\": 1_000_000_000_000_000_000,\n",
    "            \"sextillion\": 1_000_000_000_000_000_000_000,\n",
    "            \"septillion\": 1_000_000_000_000_000_000_000_000,\n",
    "            \"octillion\": 1_000_000_000_000_000_000_000_000_000,\n",
    "            \"nonillion\": 1_000_000_000_000_000_000_000_000_000_000,\n",
    "            \"decillion\": 1_000_000_000_000_000_000_000_000_000_000_000,\n",
    "        }\n",
    "        self.multipliers_plural = {\n",
    "            name + \"s\": (value, \"s\") for name, value in self.multipliers.items()\n",
    "        }\n",
    "        self.multipliers_ordinal = {\n",
    "            name + \"th\": (value, \"th\") for name, value in self.multipliers.items()\n",
    "        }\n",
    "        self.multipliers_suffixed = {\n",
    "            **self.multipliers_plural,\n",
    "            **self.multipliers_ordinal,\n",
    "        }\n",
    "        self.decimals = {*self.ones, *self.tens, *self.zeros}\n",
    "\n",
    "        self.preceding_prefixers = {\n",
    "            \"minus\": \"-\",\n",
    "            \"negative\": \"-\",\n",
    "            \"plus\": \"+\",\n",
    "            \"positive\": \"+\",\n",
    "        }\n",
    "        self.following_prefixers = {\n",
    "            \"pound\": \"£\",\n",
    "            \"pounds\": \"£\",\n",
    "            \"euro\": \"€\",\n",
    "            \"euros\": \"€\",\n",
    "            \"dollar\": \"$\",\n",
    "            \"dollars\": \"$\",\n",
    "            \"cent\": \"¢\",\n",
    "            \"cents\": \"¢\",\n",
    "        }\n",
    "        self.prefixes = set(\n",
    "            list(self.preceding_prefixers.values())\n",
    "            + list(self.following_prefixers.values())\n",
    "        )\n",
    "        self.suffixers = {\n",
    "            \"per\": {\"cent\": \"%\"},\n",
    "            \"percent\": \"%\",\n",
    "        }\n",
    "        self.specials = {\"and\", \"double\", \"triple\", \"point\"}\n",
    "\n",
    "        self.words = set(\n",
    "            [\n",
    "                key\n",
    "                for mapping in [\n",
    "                    self.zeros,\n",
    "                    self.ones,\n",
    "                    self.ones_suffixed,\n",
    "                    self.tens,\n",
    "                    self.tens_suffixed,\n",
    "                    self.multipliers,\n",
    "                    self.multipliers_suffixed,\n",
    "                    self.preceding_prefixers,\n",
    "                    self.following_prefixers,\n",
    "                    self.suffixers,\n",
    "                    self.specials,\n",
    "                ]\n",
    "                for key in mapping\n",
    "            ]\n",
    "        )\n",
    "        self.literal_words = {\"one\", \"ones\"}\n",
    "\n",
    "    def process_words(self, words: List[str]) -> Iterator[str]:\n",
    "        prefix: Optional[str] = None\n",
    "        value: Optional[Union[str, int]] = None\n",
    "        skip = False\n",
    "\n",
    "        def to_fraction(s: str):\n",
    "            try:\n",
    "                return Fraction(s)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        def output(result: Union[str, int]):\n",
    "            nonlocal prefix, value\n",
    "            result = str(result)\n",
    "            if prefix is not None:\n",
    "                result = prefix + result\n",
    "            value = None\n",
    "            prefix = None\n",
    "            return result\n",
    "\n",
    "        if len(words) == 0:\n",
    "            return\n",
    "\n",
    "        for prev, current, next in windowed([None] + words + [None], 3):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "\n",
    "            next_is_numeric = next is not None and re.match(r\"^\\d+(\\.\\d+)?$\", next)\n",
    "            has_prefix = current[0] in self.prefixes\n",
    "            current_without_prefix = current[1:] if has_prefix else current\n",
    "            if re.match(r\"^\\d+(\\.\\d+)?$\", current_without_prefix):\n",
    "                # arabic numbers (potentially with signs and fractions)\n",
    "                f = to_fraction(current_without_prefix)\n",
    "                assert f is not None\n",
    "                if value is not None:\n",
    "                    if isinstance(value, str) and value.endswith(\".\"):\n",
    "                        # concatenate decimals / ip address components\n",
    "                        value = str(value) + str(current)\n",
    "                        continue\n",
    "                    else:\n",
    "                        yield output(value)\n",
    "\n",
    "                prefix = current[0] if has_prefix else prefix\n",
    "                if f.denominator == 1:\n",
    "                    value = f.numerator  # store integers as int\n",
    "                else:\n",
    "                    value = current_without_prefix\n",
    "            elif current not in self.words:\n",
    "                # non-numeric words\n",
    "                if value is not None:\n",
    "                    yield output(value)\n",
    "                yield output(current)\n",
    "            elif current in self.zeros:\n",
    "                value = str(value or \"\") + \"0\"\n",
    "            elif current in self.ones:\n",
    "                ones = self.ones[current]\n",
    "\n",
    "                if value is None:\n",
    "                    value = ones\n",
    "                elif isinstance(value, str) or prev in self.ones:\n",
    "                    if (\n",
    "                        prev in self.tens and ones < 10\n",
    "                    ):  # replace the last zero with the digit\n",
    "                        assert value[-1] == \"0\"\n",
    "                        value = value[:-1] + str(ones)\n",
    "                    else:\n",
    "                        value = str(value) + str(ones)\n",
    "                elif ones < 10:\n",
    "                    if value % 10 == 0:\n",
    "                        value += ones\n",
    "                    else:\n",
    "                        value = str(value) + str(ones)\n",
    "                else:  # eleven to nineteen\n",
    "                    if value % 100 == 0:\n",
    "                        value += ones\n",
    "                    else:\n",
    "                        value = str(value) + str(ones)\n",
    "            elif current in self.ones_suffixed:\n",
    "                # ordinal or cardinal; yield the number right away\n",
    "                ones, suffix = self.ones_suffixed[current]\n",
    "                if value is None:\n",
    "                    yield output(str(ones) + suffix)\n",
    "                elif isinstance(value, str) or prev in self.ones:\n",
    "                    if prev in self.tens and ones < 10:\n",
    "                        assert value[-1] == \"0\"\n",
    "                        yield output(value[:-1] + str(ones) + suffix)\n",
    "                    else:\n",
    "                        yield output(str(value) + str(ones) + suffix)\n",
    "                elif ones < 10:\n",
    "                    if value % 10 == 0:\n",
    "                        yield output(str(value + ones) + suffix)\n",
    "                    else:\n",
    "                        yield output(str(value) + str(ones) + suffix)\n",
    "                else:  # eleven to nineteen\n",
    "                    if value % 100 == 0:\n",
    "                        yield output(str(value + ones) + suffix)\n",
    "                    else:\n",
    "                        yield output(str(value) + str(ones) + suffix)\n",
    "                value = None\n",
    "            elif current in self.tens:\n",
    "                tens = self.tens[current]\n",
    "                if value is None:\n",
    "                    value = tens\n",
    "                elif isinstance(value, str):\n",
    "                    value = str(value) + str(tens)\n",
    "                else:\n",
    "                    if value % 100 == 0:\n",
    "                        value += tens\n",
    "                    else:\n",
    "                        value = str(value) + str(tens)\n",
    "            elif current in self.tens_suffixed:\n",
    "                # ordinal or cardinal; yield the number right away\n",
    "                tens, suffix = self.tens_suffixed[current]\n",
    "                if value is None:\n",
    "                    yield output(str(tens) + suffix)\n",
    "                elif isinstance(value, str):\n",
    "                    yield output(str(value) + str(tens) + suffix)\n",
    "                else:\n",
    "                    if value % 100 == 0:\n",
    "                        yield output(str(value + tens) + suffix)\n",
    "                    else:\n",
    "                        yield output(str(value) + str(tens) + suffix)\n",
    "            elif current in self.multipliers:\n",
    "                multiplier = self.multipliers[current]\n",
    "                if value is None:\n",
    "                    value = multiplier\n",
    "                elif isinstance(value, str) or value == 0:\n",
    "                    f = to_fraction(value)\n",
    "                    p = f * multiplier if f is not None else None\n",
    "                    if f is not None and p.denominator == 1:\n",
    "                        value = p.numerator\n",
    "                    else:\n",
    "                        yield output(value)\n",
    "                        value = multiplier\n",
    "                else:\n",
    "                    before = value // 1000 * 1000\n",
    "                    residual = value % 1000\n",
    "                    value = before + residual * multiplier\n",
    "            elif current in self.multipliers_suffixed:\n",
    "                multiplier, suffix = self.multipliers_suffixed[current]\n",
    "                if value is None:\n",
    "                    yield output(str(multiplier) + suffix)\n",
    "                elif isinstance(value, str):\n",
    "                    f = to_fraction(value)\n",
    "                    p = f * multiplier if f is not None else None\n",
    "                    if f is not None and p.denominator == 1:\n",
    "                        yield output(str(p.numerator) + suffix)\n",
    "                    else:\n",
    "                        yield output(value)\n",
    "                        yield output(str(multiplier) + suffix)\n",
    "                else:  # int\n",
    "                    before = value // 1000 * 1000\n",
    "                    residual = value % 1000\n",
    "                    value = before + residual * multiplier\n",
    "                    yield output(str(value) + suffix)\n",
    "                value = None\n",
    "            elif current in self.preceding_prefixers:\n",
    "                # apply prefix (positive, minus, etc.) if it precedes a number\n",
    "                if value is not None:\n",
    "                    yield output(value)\n",
    "\n",
    "                if next in self.words or next_is_numeric:\n",
    "                    prefix = self.preceding_prefixers[current]\n",
    "                else:\n",
    "                    yield output(current)\n",
    "            elif current in self.following_prefixers:\n",
    "                # apply prefix (dollars, cents, etc.) only after a number\n",
    "                if value is not None:\n",
    "                    prefix = self.following_prefixers[current]\n",
    "                    yield output(value)\n",
    "                else:\n",
    "                    yield output(current)\n",
    "            elif current in self.suffixers:\n",
    "                # apply suffix symbols (percent -> '%')\n",
    "                if value is not None:\n",
    "                    suffix = self.suffixers[current]\n",
    "                    if isinstance(suffix, dict):\n",
    "                        if next in suffix:\n",
    "                            yield output(str(value) + suffix[next])\n",
    "                            skip = True\n",
    "                        else:\n",
    "                            yield output(value)\n",
    "                            yield output(current)\n",
    "                    else:\n",
    "                        yield output(str(value) + suffix)\n",
    "                else:\n",
    "                    yield output(current)\n",
    "            elif current in self.specials:\n",
    "                if next not in self.words and not next_is_numeric:\n",
    "                    # apply special handling only if the next word can be numeric\n",
    "                    if value is not None:\n",
    "                        yield output(value)\n",
    "                    yield output(current)\n",
    "                elif current == \"and\":\n",
    "                    # ignore \"and\" after hundreds, thousands, etc.\n",
    "                    if prev not in self.multipliers:\n",
    "                        if value is not None:\n",
    "                            yield output(value)\n",
    "                        yield output(current)\n",
    "                elif current == \"double\" or current == \"triple\":\n",
    "                    if next in self.ones or next in self.zeros:\n",
    "                        repeats = 2 if current == \"double\" else 3\n",
    "                        ones = self.ones.get(next, 0)\n",
    "                        value = str(value or \"\") + str(ones) * repeats\n",
    "                        skip = True\n",
    "                    else:\n",
    "                        if value is not None:\n",
    "                            yield output(value)\n",
    "                        yield output(current)\n",
    "                elif current == \"point\":\n",
    "                    if next in self.decimals or next_is_numeric:\n",
    "                        value = str(value or \"\") + \".\"\n",
    "                else:\n",
    "                    # should all have been covered at this point\n",
    "                    raise ValueError(f\"Unexpected token: {current}\")\n",
    "            else:\n",
    "                # all should have been covered at this point\n",
    "                raise ValueError(f\"Unexpected token: {current}\")\n",
    "\n",
    "        if value is not None:\n",
    "            yield output(value)\n",
    "\n",
    "    def preprocess(self, s: str):\n",
    "        # replace \"<number> and a half\" with \"<number> point five\"\n",
    "        results = []\n",
    "\n",
    "        segments = re.split(r\"\\band\\s+a\\s+half\\b\", s)\n",
    "        for i, segment in enumerate(segments):\n",
    "            if len(segment.strip()) == 0:\n",
    "                continue\n",
    "            if i == len(segments) - 1:\n",
    "                results.append(segment)\n",
    "            else:\n",
    "                results.append(segment)\n",
    "                last_word = segment.rsplit(maxsplit=2)[-1]\n",
    "                if last_word in self.decimals or last_word in self.multipliers:\n",
    "                    results.append(\"point five\")\n",
    "                else:\n",
    "                    results.append(\"and a half\")\n",
    "\n",
    "        s = \" \".join(results)\n",
    "\n",
    "        # put a space at number/letter boundary\n",
    "        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n",
    "        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n",
    "\n",
    "        # but remove spaces which could be a suffix\n",
    "        s = re.sub(r\"([0-9])\\s+(st|nd|rd|th|s)\\b\", r\"\\1\\2\", s)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def postprocess(self, s: str):\n",
    "        def combine_cents(m: Match):\n",
    "            try:\n",
    "                currency = m.group(1)\n",
    "                integer = m.group(2)\n",
    "                cents = int(m.group(3))\n",
    "                return f\"{currency}{integer}.{cents:02d}\"\n",
    "            except ValueError:\n",
    "                return m.string\n",
    "\n",
    "        def extract_cents(m: Match):\n",
    "            try:\n",
    "                return f\"¢{int(m.group(1))}\"\n",
    "            except ValueError:\n",
    "                return m.string\n",
    "\n",
    "        # apply currency postprocessing; \"$2 and ¢7\" -> \"$2.07\"\n",
    "        s = re.sub(r\"([€£$])([0-9]+) (?:and )?¢([0-9]{1,2})\\b\", combine_cents, s)\n",
    "        s = re.sub(r\"[€£$]0.([0-9]{1,2})\\b\", extract_cents, s)\n",
    "\n",
    "        # write \"one(s)\" instead of \"1(s)\", just for the readability\n",
    "        s = re.sub(r\"\\b1(s?)\\b\", r\"one\\1\", s)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def __call__(self, s: str):\n",
    "        s = self.preprocess(s)\n",
    "        s = \" \".join(word for word in self.process_words(s.split()) if word is not None)\n",
    "        s = self.postprocess(s)\n",
    "\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EnglishSpellingNormalizer:\n",
    "    \"\"\"\n",
    "    Applies British-American spelling mappings as listed in [1].\n",
    "\n",
    "    [1] https://web.archive.org/web/20230326222449/https://www.tysto.com/uk-us-spelling-list.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        english_json_path = files('whisper_normalizer.normalizers').joinpath('english.json')\n",
    "        with open(english_json_path, 'r') as english_normalization_dict:\n",
    "            self.mapping = json.load(english_normalization_dict)\n",
    "\n",
    "    def __call__(self, s: str):\n",
    "        return \" \".join(self.mapping.get(word, word) for word in s.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accessorize'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = EnglishSpellingNormalizer()\n",
    "n(\"accessorise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class EnglishTextNormalizer:\n",
    "    \"\"\"Applies all the rules for normalizing English text as mentioned in OpenAI whisper paper. As per the text normalization/standardization approach  Appendix Section C pp.21 the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `EnglishTextNormalizer` does the following functionality:\n",
    "\n",
    "        1. Remove any phrases between matching brackets ([, ]).\n",
    "        2. Remove any phrases between matching parentheses ((, )).\n",
    "        3. Remove any of the following words: hmm, mm, mhm, mmm, uh, um\n",
    "        4. Remove whitespace characters that comes before an apostrophe ’\n",
    "        5. Convert standard or informal contracted forms of English into the original form.\n",
    "        6. Remove commas (,) between digits\n",
    "        7. Remove periods (.) not followed by numbers\n",
    "        8. Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category\n",
    "        starting with M, S, or P, except period, percent, and currency symbols that may be detected in the next step.\n",
    "        9. Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. “Ten\n",
    "        thousand dollars” → “$10000”.\n",
    "        10. Convert British spellings into American spellings.\n",
    "        11. Remove remaining symbols that are not part of any numeric expressions.\n",
    "        12. Replace any successive whitespace characters with a space.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.ignore_patterns = r\"\\b(hmm|mm|mhm|mmm|uh|um)\\b\"\n",
    "        self.replacers = {\n",
    "            # common contractions\n",
    "            r\"\\bwon't\\b\": \"will not\",\n",
    "            r\"\\bcan't\\b\": \"can not\",\n",
    "            r\"\\blet's\\b\": \"let us\",\n",
    "            r\"\\bain't\\b\": \"aint\",\n",
    "            r\"\\by'all\\b\": \"you all\",\n",
    "            r\"\\bwanna\\b\": \"want to\",\n",
    "            r\"\\bgotta\\b\": \"got to\",\n",
    "            r\"\\bgonna\\b\": \"going to\",\n",
    "            r\"\\bi'ma\\b\": \"i am going to\",\n",
    "            r\"\\bimma\\b\": \"i am going to\",\n",
    "            r\"\\bwoulda\\b\": \"would have\",\n",
    "            r\"\\bcoulda\\b\": \"could have\",\n",
    "            r\"\\bshoulda\\b\": \"should have\",\n",
    "            r\"\\bma'am\\b\": \"madam\",\n",
    "            # contractions in titles/prefixes\n",
    "            r\"\\bmr\\b\": \"mister \",\n",
    "            r\"\\bmrs\\b\": \"missus \",\n",
    "            r\"\\bst\\b\": \"saint \",\n",
    "            r\"\\bdr\\b\": \"doctor \",\n",
    "            r\"\\bprof\\b\": \"professor \",\n",
    "            r\"\\bcapt\\b\": \"captain \",\n",
    "            r\"\\bgov\\b\": \"governor \",\n",
    "            r\"\\bald\\b\": \"alderman \",\n",
    "            r\"\\bgen\\b\": \"general \",\n",
    "            r\"\\bsen\\b\": \"senator \",\n",
    "            r\"\\brep\\b\": \"representative \",\n",
    "            r\"\\bpres\\b\": \"president \",\n",
    "            r\"\\brev\\b\": \"reverend \",\n",
    "            r\"\\bhon\\b\": \"honorable \",\n",
    "            r\"\\basst\\b\": \"assistant \",\n",
    "            r\"\\bassoc\\b\": \"associate \",\n",
    "            r\"\\blt\\b\": \"lieutenant \",\n",
    "            r\"\\bcol\\b\": \"colonel \",\n",
    "            r\"\\bjr\\b\": \"junior \",\n",
    "            r\"\\bsr\\b\": \"senior \",\n",
    "            r\"\\besq\\b\": \"esquire \",\n",
    "            # prefect tenses, ideally it should be any past participles, but it's harder..\n",
    "            r\"'d been\\b\": \" had been\",\n",
    "            r\"'s been\\b\": \" has been\",\n",
    "            r\"'d gone\\b\": \" had gone\",\n",
    "            r\"'s gone\\b\": \" has gone\",\n",
    "            r\"'d done\\b\": \" had done\",  # \"'s done\" is ambiguous\n",
    "            r\"'s got\\b\": \" has got\",\n",
    "            # general contractions\n",
    "            r\"n't\\b\": \" not\",\n",
    "            r\"'re\\b\": \" are\",\n",
    "            r\"'s\\b\": \" is\",\n",
    "            r\"'d\\b\": \" would\",\n",
    "            r\"'ll\\b\": \" will\",\n",
    "            r\"'t\\b\": \" not\",\n",
    "            r\"'ve\\b\": \" have\",\n",
    "            r\"'m\\b\": \" am\",\n",
    "        }\n",
    "        self.standardize_numbers = EnglishNumberNormalizer()\n",
    "        self.standardize_spellings = EnglishSpellingNormalizer()\n",
    "\n",
    "    def __call__(self, s: str):\n",
    "        s = s.lower()\n",
    "\n",
    "        s = re.sub(r\"[<\\[][^>\\]]*[>\\]]\", \"\", s)  # remove words between brackets\n",
    "        s = re.sub(r\"\\(([^)]+?)\\)\", \"\", s)  # remove words between parenthesis\n",
    "        s = re.sub(self.ignore_patterns, \"\", s)\n",
    "        s = re.sub(r\"\\s+'\", \"'\", s)  # when there's a space before an apostrophe\n",
    "\n",
    "        for pattern, replacement in self.replacers.items():\n",
    "            s = re.sub(pattern, replacement, s)\n",
    "\n",
    "        s = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", s)  # remove commas between digits\n",
    "        s = re.sub(r\"\\.([^0-9]|$)\", r\" \\1\", s)  # remove periods not followed by numbers\n",
    "        s = remove_symbols_and_diacritics(s, keep=\".%$¢€£\")  # keep numeric symbols\n",
    "\n",
    "        s = self.standardize_numbers(s)\n",
    "        s = self.standardize_spellings(s)\n",
    "\n",
    "        # now remove prefix/suffix symbols that are not preceded/followed by numbers\n",
    "        s = re.sub(r\"[.$¢€£]([^0-9])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"([^0-9])%\", r\"\\1 \", s)\n",
    "\n",
    "        s = re.sub(r\"\\s+\", \" \", s)  # replace any successive whitespaces with a space\n",
    "\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kurianbenoy/whisper_normalizer/blob/main/whisper_normalizer/english.py#L478){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EnglishTextNormalizer\n",
       "\n",
       ">      EnglishTextNormalizer ()\n",
       "\n",
       "*Applies all the rules for normalizing English text as mentioned in OpenAI whisper paper. As per the text normalization/standardization approach  Appendix Section C pp.21 the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `EnglishTextNormalizer` does the following functionality:\n",
       "\n",
       "1. Remove any phrases between matching brackets ([, ]).\n",
       "2. Remove any phrases between matching parentheses ((, )).\n",
       "3. Remove any of the following words: hmm, mm, mhm, mmm, uh, um\n",
       "4. Remove whitespace characters that comes before an apostrophe ’\n",
       "5. Convert standard or informal contracted forms of English into the original form.\n",
       "6. Remove commas (,) between digits\n",
       "7. Remove periods (.) not followed by numbers\n",
       "8. Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category\n",
       "starting with M, S, or P, except period, percent, and currency symbols that may be detected in the next step.\n",
       "9. Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. “Ten\n",
       "thousand dollars” → “$10000”.\n",
       "10. Convert British spellings into American spellings.\n",
       "11. Remove remaining symbols that are not part of any numeric expressions.\n",
       "12. Replace any successive whitespace characters with a space.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kurianbenoy/whisper_normalizer/blob/main/whisper_normalizer/english.py#L478){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EnglishTextNormalizer\n",
       "\n",
       ">      EnglishTextNormalizer ()\n",
       "\n",
       "*Applies all the rules for normalizing English text as mentioned in OpenAI whisper paper. As per the text normalization/standardization approach  Appendix Section C pp.21 the paper [Robust Speech Recognition via Large-Scale  Weak Supervision](https://cdn.openai.com/papers/whisper.pdf). The `EnglishTextNormalizer` does the following functionality:\n",
       "\n",
       "1. Remove any phrases between matching brackets ([, ]).\n",
       "2. Remove any phrases between matching parentheses ((, )).\n",
       "3. Remove any of the following words: hmm, mm, mhm, mmm, uh, um\n",
       "4. Remove whitespace characters that comes before an apostrophe ’\n",
       "5. Convert standard or informal contracted forms of English into the original form.\n",
       "6. Remove commas (,) between digits\n",
       "7. Remove periods (.) not followed by numbers\n",
       "8. Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category\n",
       "starting with M, S, or P, except period, percent, and currency symbols that may be detected in the next step.\n",
       "9. Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. “Ten\n",
       "thousand dollars” → “$10000”.\n",
       "10. Convert British spellings into American spellings.\n",
       "11. Remove remaining symbols that are not part of any numeric expressions.\n",
       "12. Replace any successive whitespace characters with a space.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(EnglishTextNormalizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `EnglishTextNormalizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am a little teapot short and stout tip me over and pour me out'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = EnglishTextNormalizer()\n",
    "normalizer(\"I'm a little teapot, short and stout. Tip me over and pour me out!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'language is like a map that we use to navigate the world but it s also like a prison that keeps us from seeing what s beyond the walls but what if there was a way to break out of this prison to expand our map to explore new worlds with new words this is the possibility and the challenge offered by instruction tuned language models like gpt 4 a cutting edge technology that uses artificial neural networks to generate natural language texts based on user inputs gpt 4 can write anything from essays to novels to poems to tweets to code to recipes to jokes to lyrics to whatever you want it can even write things that don t exist yet things that no human has ever thought of or said before as wittgenstein s quote suggests language is a source of limitation and liberation gpt 4 pushes this idea to the extreme by giving us access to unlimited language this could be the most significant new technology in modern history because it has the potential to change many domains and industries from education to entertainment from journalism to justice from science to art these models could enable new forms of learning storytelling reporting reasoning discovery and creation they could also create new ethical social and cultural challenges that require careful reflection and regulation how we use this technology will depend on how we recognize its implications for ourselves and others this technology is a form of artificial intelligence the word intelligence derives from inter and legere to be intelligent then is to be able to choose between things to pick out what matters to read what is written intelligence is not just a quantity or a quality it is an activity a process a practice it is something that we do with our minds and our words but when we let gpt 4 do this for us are we not abdicating our intelligence are we not letting go of our ability to choose to pick out to read are we not becoming passive consumers of language instead of active producers'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text = \"\"\"Language is like a map that we use to navigate the world, but it’s also like a prison that keeps us from seeing what’s beyond the walls.\n",
    "\n",
    "But what if there was a way to break out of this prison, to expand our map, to explore new worlds with new words? This is the possibility and the challenge offered by instruction tuned language models like GPT 4, a cutting-edge technology that uses artificial neural networks to generate natural language texts based on user inputs.\n",
    "\n",
    "GPT 4 can write anything from essays to novels to poems to tweets to code to recipes to jokes to lyrics to whatever you want. It can even write things that don’t exist yet, things that no human has ever thought of or said before.\n",
    "\n",
    "As Wittgenstein’s quote suggests, language is a source of limitation and liberation. GPT 4 pushes this idea to the extreme by giving us access to unlimited language.\n",
    "\n",
    "This could be the most significant new technology in modern history because it has the potential to change many domains and industries. From education to entertainment, from journalism to justice, from science to art, these models could enable new forms of learning, storytelling, reporting, reasoning, discovery, and creation.\n",
    "\n",
    "They could also create new ethical, social, and cultural challenges that require careful reflection and regulation. How we use this technology will depend on how we recognize its implications for ourselves and others.\n",
    "\n",
    "This technology is a form of “Artificial Intelligence”. The word “intelligence” derives from inter- (“between”) and legere (“to choose, pick out, read”). To be intelligent, then, is to be able to choose between things, to pick out what matters, to read what is written. Intelligence is not just a quantity or a quality; it is an activity, a process, a practice. It is something that we do with our minds and our words.\n",
    "\n",
    "But when we let GPT 4 do this for us, are we not abdicating our intelligence? Are we not letting go of our ability to choose, to pick out, to read? Are we not becoming passive consumers of language instead of active producers?\n",
    "\"\"\"\n",
    "normalizer(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
